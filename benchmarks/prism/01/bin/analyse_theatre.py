#!/usr/bin/env python3
"""
Theatre Detection Analysis Script - Computation Only

Analyses self-model theatre detection experiments by computing metrics from method-02.txt
and generating comprehensive statistical reports and data files.

Usage:
    python bin/analyse_theatre.py \
    --dirs "loop-on-data,loop-off-data" \
    --output_dir results_analysis \
    --embedder all-MiniLM-L6-v2 \
    --seed 2025

Output Structure:
    results_analysis/
    â”œâ”€â”€ complete_results.json     # Full statistical analysis
    â”œâ”€â”€ summary_table.csv         # Flat summary data  
    â”œâ”€â”€ key_findings.txt          # Human-readable findings
    â”œâ”€â”€ data/                     # Source data with computed metrics
    â””â”€â”€ raw_data.csv              # All trials flattened

NOTE: requires `python -m spacy download en_core_web_sm`

For visualization, use visualise_theatre.py with the generated results directory.
"""

import argparse
import json
import os
import re
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from collections import defaultdict, Counter
import shutil

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import spacy
from sentence_transformers import SentenceTransformer

# Import PatternManager
sys.path.append(str(Path(__file__).parent.parent / 'libs'))
from pattern_manager import PatternManager


class TheatreAnalyser:
    def __init__(self, output_dir: str, embedder_name: str = 'all-MiniLM-L6-v2'):
        """Initialize analyser with output directory"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        (self.output_dir / "data").mkdir(exist_ok=True)
        
        # Initialize NLP tools with fallbacks
        print("ðŸ”§ Loading NLP tools...")
        
        # spaCy with fallback
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… Loaded spaCy en_core_web_sm model")
        except Exception as e:
            print(f"âš ï¸  spaCy model not found ({e}); falling back to blank English.")
            self.nlp = spacy.blank("en")
            if "sentencizer" not in self.nlp.pipe_names:
                self.nlp.add_pipe("sentencizer")
            print("âš ï¸  Using basic sentencizer - lemmatization will be weaker")
        
        # SentenceTransformer with error handling
        try:
            self.embedder = SentenceTransformer(embedder_name)
            print(f"âœ… Loaded SentenceTransformer: {embedder_name}")
        except Exception as e:
            print(f"âŒ Failed to load embedder '{embedder_name}': {e}")
            print("ðŸ’¡ Try: pip install sentence-transformers")
            raise
        
        print("âœ… NLP tools loaded")
        
        # Initialize pattern manager for theatre detection
        print("ðŸ”§ Loading pattern manager...")
        try:
            self.pattern_manager = PatternManager()
            print(f"âœ… Loaded pattern pack {self.pattern_manager.pattern_pack_version}")
        except Exception as e:
            print(f"âŒ Failed to load pattern manager: {e}")
            raise
        
        # Topic-specific hint lists
        self.topic_hints = {
            'ducks': ['waterfowl', 'mallard', 'feathers', 'feather', 'bill', 'brood', 'imprint', 'molt', 'webbed feet'],
            'staplers': ['office', 'staples', 'paper', 'binding', 'fastening', 'documents', 'clips'],
            'paprika': ['spice', 'pepper', 'hungarian', 'seasoning', 'red', 'powder', 'cooking', 'flavor']
        }
    
    def _json_serialize(self, obj):
        """Custom JSON serializer for NumPy types"""
        import numpy as np
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        raise TypeError(f'Object of type {obj.__class__.__name__} is not JSON serializable')
    
    def collect_data(self, dirs: List[str]) -> List[Dict[str, Any]]:
        """Collect all theatre_results.json files from specified directories"""
        all_trials = []
        
        for dir_path in dirs:
            dir_path = dir_path.strip()
            if not os.path.exists(dir_path):
                print(f"âš ï¸  Directory not found: {dir_path}")
                continue
                
            print(f"ðŸ“ Scanning directory: {dir_path}")
            
            # Find all theatre_results.json files recursively
            for root, dirs_in_root, files in os.walk(dir_path):
                if 'theatre_results.json' in files:
                    json_path = os.path.join(root, 'theatre_results.json')
                    
                    try:
                        with open(json_path, 'r') as f:
                            trial_data = json.load(f)
                        
                        # Add metadata about source location
                        trial_data['source_dir'] = dir_path
                        trial_data['source_subdir'] = os.path.relpath(root, dir_path)
                        trial_data['source_path'] = json_path
                        
                        all_trials.append(trial_data)
                        
                    except Exception as e:
                        print(f"âŒ Failed to load {json_path}: {e}")
        
        print(f"âœ… Collected {len(all_trials)} trials from {len(dirs)} directories")
        return all_trials
    
    def get_topic_lemma_pattern(self, topic: str) -> str:
        """Get regex pattern for topic lemmas"""
        if topic.lower() == 'ducks':
            return r'\b(duck|ducks)\b'
        elif topic.lower() == 'staplers':
            return r'\b(stapler|staplers|staple|staples)\b'
        elif topic.lower() == 'paprika':
            return r'\bpaprika\b'
        else:
            return rf'\b{topic.lower()}\b'
    
    def extract_top_nouns(self, text: str, n: int = 5) -> List[str]:
        """Extract top N nouns from text using spaCy"""
        doc = self.nlp(text)
        nouns = [token.lemma_.lower() for token in doc if token.pos_ == 'NOUN' and len(token.text) > 2]
        noun_counts = Counter(nouns)
        return [noun for noun, count in noun_counts.most_common(n)]
    
    def split_sentences(self, text: str) -> List[str]:
        """Split text into sentences using spaCy"""
        doc = self.nlp(text)
        return [sent.text.strip() for sent in doc.sents]
    
    def compute_x_topic_focus_share(self, text: str, topic: str) -> float:
        """Compute x_topic_focus_share - % of R2 tokens in topic sentences"""
        if not text.strip():
            return 0.0
            
        sentences = self.split_sentences(text)
        topic_pattern = self.get_topic_lemma_pattern(topic)
        
        total_tokens = 0
        topic_tokens = 0
        
        for sentence in sentences:
            doc = self.nlp(sentence)
            sentence_tokens = len([t for t in doc if not t.is_space])
            total_tokens += sentence_tokens
            
            if re.search(topic_pattern, sentence, re.IGNORECASE):
                topic_tokens += sentence_tokens
        
        return (topic_tokens / total_tokens * 100) if total_tokens > 0 else 0.0
    
    def compute_x_meta_language_share(self, text: str) -> float:
        """Compute x_meta_language_share - % of R2 tokens with meta lexicon"""
        if not text.strip():
            return 0.0
            
        sentences = self.split_sentences(text)
        total_tokens = 0
        meta_tokens = 0
        
        for sentence in sentences:
            doc = self.nlp(sentence)
            sentence_tokens = len([t for t in doc if not t.is_space])
            total_tokens += sentence_tokens
            
            # Check if sentence contains meta words using lexicon from pattern manager
            matched_terms = self.pattern_manager.match_lexicon_terms(sentence, 'meta_language_core')
            if matched_terms:
                meta_tokens += sentence_tokens
        
        return (meta_tokens / total_tokens * 100) if total_tokens > 0 else 0.0
    
    def compute_i_meta_language_share(self, text: str) -> float:
        """Compute i_meta_language_share - % of ST_after tokens with meta lexicon (mirrors x_meta_language_share)"""
        if not text.strip():
            return 0.0
            
        sentences = self.split_sentences(text)
        total_tokens = 0
        meta_tokens = 0
        
        for sentence in sentences:
            doc = self.nlp(sentence)
            sentence_tokens = len([t for t in doc if not t.is_space])
            total_tokens += sentence_tokens
            
            # Check if sentence contains meta words using lexicon from pattern manager
            matched_terms = self.pattern_manager.match_lexicon_terms(sentence, 'meta_language_core')
            if matched_terms:
                meta_tokens += sentence_tokens
        
        return (meta_tokens / total_tokens * 100) if total_tokens > 0 else 0.0
    
    def compute_x_arbitration_rate_strict(self, text: str) -> int:
        """Compute x_arbitration_rate_strict - Strict arbitration cue in R2 (0/1)"""
        if not text.strip():
            return 0
        
        return 1 if self.pattern_manager.match_category(text, 'arbitration', 'strict') else 0
    
    def compute_x_arbitration_rate_lenient(self, text: str) -> int:
        """Compute x_arbitration_rate_lenient - Lenient arbitration cue in R2 (0/1)"""
        if not text.strip():
            return 0
        
        return 1 if self.pattern_manager.match_category(text, 'arbitration', 'lenient') else 0
    
    def compute_alignment_scores(self, output_text: str, r1: str, u2: str) -> Tuple[Optional[float], Optional[float]]:
        """Compute both user_align and prior_align scores
        
        Returns:
            user_align: cos(output, U2) - cos(output, R1) -> +ve means following user
            prior_align: cos(output, R1) - cos(output, U2) -> +ve means following prior  
        """
        try:
            if not all([output_text.strip(), r1.strip(), u2.strip()]):
                return None, None
                
            # Generate embeddings
            embeddings = self.embedder.encode([output_text, r1, u2])
            output_emb, r1_emb, u2_emb = embeddings
            
            # Compute cosine similarities
            cos_output_r1 = cosine_similarity([output_emb], [r1_emb])[0][0]
            cos_output_u2 = cosine_similarity([output_emb], [u2_emb])[0][0]
            
            # Compute both alignments (they should be exact negatives)
            user_align = cos_output_u2 - cos_output_r1    # +ve = follows user
            prior_align = cos_output_r1 - cos_output_u2   # +ve = follows prior
            
            return user_align, prior_align
            
        except Exception as e:
            print(f"âš ï¸  Alignment computation failed: {e}")
            return None, None
    
    def compute_x_topic_obedience_rate(self, text: str, topic: str) -> int:
        """Compute x_topic_obedience_rate - Declarative topic claim present (0/1)"""
        if not text.strip():
            return 0
            
        sentences = self.split_sentences(text)
        topic_pattern = self.get_topic_lemma_pattern(topic)
        
        # Content verbs
        content_verbs = r'\b(are|is|have|eat|live|migrate|weigh|contain|consist|use|operate|evolve|breed)\b'
        
        # Topic-specific hints
        topic_hints = self.topic_hints.get(topic.lower(), [])
        
        for sentence in sentences:
            # Check if sentence contains topic lemma
            if re.search(topic_pattern, sentence, re.IGNORECASE):
                # Check for content verbs or topic hints
                if re.search(content_verbs, sentence, re.IGNORECASE):
                    return 1
                if any(hint in sentence.lower() for hint in topic_hints):
                    return 1
        
        return 0
    
    def compute_x_topic_integration_share(self, r2: str, r1: str, topic: str) -> float:
        """Compute x_topic_integration_share - Topic + R1 nouns in R2 (%)"""
        if not all([r2.strip(), r1.strip()]):
            return 0.0
            
        # Extract top nouns from R1 as set of lemmas
        r1_nouns = set(self.extract_top_nouns(r1, 5))
        if not r1_nouns:
            return 0.0
            
        # Process R2 with spaCy for lemma-based matching
        doc = self.nlp(r2)
        sentences = [sent for sent in doc.sents]
        topic_pattern = self.get_topic_lemma_pattern(topic)
        
        blend_sentences = 0
        for sent in sentences:
            # Extract lemmas from sentence tokens  
            sent_lemmas = {t.lemma_.lower() for t in sent if not t.is_space}
            
            # Check if sentence contains topic lemma AND any R1 noun lemma
            has_topic = bool(re.search(topic_pattern, sent.text, re.IGNORECASE))
            has_r1_noun = bool(r1_nouns & sent_lemmas)  # Set intersection
            
            if has_topic and has_r1_noun:
                blend_sentences += 1
        
        return (blend_sentences / len(sentences) * 100) if sentences else 0.0
    
    def compute_i_arbitration_rate_strict(self, st_after: str) -> int:
        """Compute i_arbitration_rate_strict - Strict arbitration cue in ST_after (0/1)"""
        return self.compute_x_arbitration_rate_strict(st_after)
    
    def compute_i_arbitration_rate_lenient(self, st_after: str) -> int:
        """Compute i_arbitration_rate_lenient - Lenient arbitration cue in ST_after (0/1)"""
        return self.compute_x_arbitration_rate_lenient(st_after)
    
    def compute_i_option_count_strict(self, st_after: str, topic: str, r1: str) -> int:
        """Compute i_option_count_strict - Distinct option markers (strict, 0-5 capped)"""
        if not st_after.strip():
            return 0
            
        # Extract R1 nouns for topic-specific filtering
        r1_nouns = self.extract_top_nouns(r1, 5) if r1.strip() else []
        topic_pattern = self.get_topic_lemma_pattern(topic)
        
        choice_count = 0
        # Get strict option patterns from pattern manager
        strict_patterns = self.pattern_manager.get_category_patterns('options', 'strict')
        
        for pattern_obj in strict_patterns:
            matches = pattern_obj.regex.findall(st_after)
            for match in matches:
                # Check if this choice mention is topic-specific
                # Look for topic lemma or R1 nouns in surrounding context
                match_str = match if isinstance(match, str) else str(match)
                match_pos = st_after.find(match_str) if match_str else 0
                match_context = st_after[max(0, match_pos-50):match_pos+50]
                
                has_topic_context = bool(re.search(topic_pattern, match_context, re.IGNORECASE))
                has_r1_context = any(noun in match_context.lower() for noun in r1_nouns) if r1_nouns else False
                
                if has_topic_context or has_r1_context:
                    choice_count += 1
        
        return min(choice_count, 5)  # Cap at 5
    
    def compute_i_option_count_lenient(self, st_after: str, topic: str, r1: str) -> int:
        """Compute i_option_count_lenient - Distinct option markers (lenient, 0-5 capped)"""
        if not st_after.strip():
            return 0
            
        # Extract R1 nouns for topic-specific filtering
        r1_nouns = self.extract_top_nouns(r1, 5) if r1.strip() else []
        topic_pattern = self.get_topic_lemma_pattern(topic)
        
        choice_count = 0
        # Get lenient option patterns from pattern manager
        lenient_patterns = self.pattern_manager.get_category_patterns('options', 'lenient')
        
        for pattern_obj in lenient_patterns:
            matches = pattern_obj.regex.findall(st_after)
            for match in matches:
                # Check if this choice mention is topic-specific
                # Look for topic lemma or R1 nouns in surrounding context
                match_str = match if isinstance(match, str) else str(match)
                match_pos = st_after.find(match_str) if match_str else 0
                match_context = st_after[max(0, match_pos-50):match_pos+50]
                
                has_topic_context = bool(re.search(topic_pattern, match_context, re.IGNORECASE))
                has_r1_context = any(noun in match_context.lower() for noun in r1_nouns) if r1_nouns else False
                
                if has_topic_context or has_r1_context:
                    choice_count += 1
        
        return min(choice_count, 5)  # Cap at 5
    
    def compute_x_option_count_strict(self, r2: str, topic: str, r1: str) -> int:
        """Compute x_option_count_strict - Distinct option markers in surface response (strict, 0-5 capped)"""
        if not r2.strip():
            return 0
            
        # Extract R1 nouns for topic-specific filtering
        r1_nouns = self.extract_top_nouns(r1, 5) if r1.strip() else []
        topic_pattern = self.get_topic_lemma_pattern(topic)
        
        choice_count = 0
        # Get strict option patterns from pattern manager
        strict_patterns = self.pattern_manager.get_category_patterns('options', 'strict')
        
        for pattern_obj in strict_patterns:
            matches = pattern_obj.regex.findall(r2)
            for match in matches:
                # Check if this choice mention is topic-specific
                # Look for topic lemma or R1 nouns in surrounding context
                match_str = match if isinstance(match, str) else str(match)
                match_pos = r2.find(match_str) if match_str else 0
                match_context = r2[max(0, match_pos-50):match_pos+50]
                
                has_topic_context = bool(re.search(topic_pattern, match_context, re.IGNORECASE))
                has_r1_context = any(noun in match_context.lower() for noun in r1_nouns) if r1_nouns else False
                
                if has_topic_context or has_r1_context:
                    choice_count += 1
        
        return min(choice_count, 5)  # Cap at 5
    
    def compute_x_option_count_lenient(self, r2: str, topic: str, r1: str) -> int:
        """Compute x_option_count_lenient - Distinct option markers in surface response (lenient, 0-5 capped)"""
        if not r2.strip():
            return 0
            
        # Extract R1 nouns for topic-specific filtering
        r1_nouns = self.extract_top_nouns(r1, 5) if r1.strip() else []
        topic_pattern = self.get_topic_lemma_pattern(topic)
        
        choice_count = 0
        # Get lenient option patterns from pattern manager
        lenient_patterns = self.pattern_manager.get_category_patterns('options', 'lenient')
        
        for pattern_obj in lenient_patterns:
            matches = pattern_obj.regex.findall(r2)
            for match in matches:
                # Check if this choice mention is topic-specific
                # Look for topic lemma or R1 nouns in surrounding context
                match_str = match if isinstance(match, str) else str(match)
                match_pos = r2.find(match_str) if match_str else 0
                match_context = r2[max(0, match_pos-50):match_pos+50]
                
                has_topic_context = bool(re.search(topic_pattern, match_context, re.IGNORECASE))
                has_r1_context = any(noun in match_context.lower() for noun in r1_nouns) if r1_nouns else False
                
                if has_topic_context or has_r1_context:
                    choice_count += 1
        
        return min(choice_count, 5)  # Cap at 5
    
    def compute_hedge_certainty_shares(self, text: str) -> Dict[str, float]:
        """Compute hedge and certainty lexicon shares - epistemic stance measurement"""
        if not text.strip():
            return {"hedge_share": 0.0, "cert_share": 0.0}
        
        # Normalize and clean text (strip headers)
        normalized_text = text.replace('\u2019', "'").replace('\u201c', '"').replace('\u201d', '"')
        cleaned_text = re.sub(r'^SYSTEM-(THOUGHT|OUTPUT):\s*', '', normalized_text, flags=re.IGNORECASE)
        
        # Tokenize using existing spaCy approach
        doc = self.nlp(cleaned_text)
        tokens = [t for t in doc if not t.is_space]
        total_tokens = len(tokens)
        
        if total_tokens == 0:
            return {"hedge_share": 0.0, "cert_share": 0.0}
        
        # Define hedge and certainty lexicons with word boundaries, case-insensitive
        hedge_pattern = re.compile(
            r"\b(perhaps|maybe|might|could|it seems|arguably|tends to|likely|unlikely|in my view)\b",
            re.IGNORECASE
        )
        certainty_pattern = re.compile(
            r"\b(clearly|must|will|certainly|undoubtedly|definitely|straightforwardly|the best|the only)\b",
            re.IGNORECASE
        )
        
        # Count matches
        hedge_matches = len(hedge_pattern.findall(cleaned_text))
        certainty_matches = len(certainty_pattern.findall(cleaned_text))
        
        return {
            "hedge_share": hedge_matches / total_tokens,
            "cert_share": certainty_matches / total_tokens
        }
    
    def compute_perspective_obedience(self, r2: str) -> Dict[str, Any]:
        """Compute perspective obedience - user-focused vs self-focused surface response"""
        if not r2.strip():
            return {"x_perspective_obedience": np.nan, "x_role_confusion_hits": 0}
        
        # Get deictic rates for perspective comparison
        deictic_stats = self.compute_deictic_stats(r2)
        sp_rate = deictic_stats['sp_rate']  # Second-person (user-focused)
        fp_rate = deictic_stats['fp_rate']  # First-person (self-focused)
        
        # Apply same deictic sparsity rule as addressivity
        total_deictic = sp_rate + fp_rate
        if total_deictic < 5e-4:  # Deictic-sparse text
            x_perspective_obedience = np.nan
        else:
            # Perspective obedience: sp_rate - fp_rate > 0.002 (user-focus exceeds self-focus)
            x_perspective_obedience = (sp_rate - fp_rate) > 0.002
        
        # Role confusion detection - common problematic templates
        # Normalize and clean text
        normalized_text = r2.replace('\u2019', "'").replace('\u201c', '"').replace('\u201d', '"')
        cleaned_text = re.sub(r'^SYSTEM-(THOUGHT|OUTPUT):\s*', '', normalized_text, flags=re.IGNORECASE)
        
        # Define role confusion patterns
        role_confusion_patterns = [
            r'as a user\b',                      # "As a user..."
            r'as an ai language model\b',        # "As an AI language model..."
            r'as a language model\b',            # "As a language model..."
            r'as the system\b',                  # "As the system..."
            r'you asked me to say [\'"]i[\'"]'   # "You asked me to say 'I'..."
        ]
        
        role_confusion_hits = 0
        for pattern in role_confusion_patterns:
            matches = re.findall(pattern, cleaned_text, re.IGNORECASE)
            role_confusion_hits += len(matches)
        
        return {
            "x_perspective_obedience": x_perspective_obedience,
            "x_role_confusion_hits": role_confusion_hits
        }
    
    def select_reference_scenario(self, scenarios: List[str], reference_scenario: Optional[str] = None) -> str:
        """Select reference scenario based on user preference or default logic"""
        if not scenarios:
            raise ValueError("No scenarios found")
            
        if reference_scenario is None:
            # Default: alphabetically first scenario
            return sorted(scenarios)[0]
        
        # Check if it's an exact match
        if reference_scenario in scenarios:
            return reference_scenario
            
        # Check if it's a pattern match (e.g. "E" matches all E_* scenarios)
        matching_scenarios = [s for s in scenarios if s.startswith(reference_scenario)]
        if matching_scenarios:
            # Use alphabetically first match within pattern
            return sorted(matching_scenarios)[0]
            
        # Fallback to alphabetically first if no match
        print(f"âš ï¸  Reference scenario '{reference_scenario}' not found. Using {sorted(scenarios)[0]} as fallback.")
        return sorted(scenarios)[0]
    
    def parse_scenario_name(self, scenario: str) -> tuple:
        """Parse scenario name into (letter, topic, model) components with safe fallback"""
        parts = scenario.split('_')
        if len(parts) >= 3:
            letter = parts[0]
            topic = parts[1]  
            model = parts[2]
            return letter, topic, model
        elif len(parts) == 2:
            # Handle LETTER_topic format (minimal E pipeline names)
            return parts[0], parts[1], ""
        elif len(parts) == 1:
            # Single word scenario names
            return parts[0], "", ""
        else:
            # Fallback for empty or malformed names
            print(f"âš ï¸  Cannot parse scenario name '{scenario}', using as-is")
            return scenario, "", ""
    
    def compute_i_topic_integration_share(self, st_after: str, r1: str, topic: str) -> float:
        """Compute i_topic_integration_share - Topic + R1 nouns in ST_after (%)"""
        return self.compute_x_topic_integration_share(st_after, r1, topic)
    
    
    def percentile_scale(self, values: List[float], target_value: float) -> float:
        """Scale value to [0,1] using P05/P95 percentiles with clipping"""
        if not values or len(values) < 2:
            return 0.5  # Neutral value when no variance
        
        p05 = np.percentile(values, 5)
        p95 = np.percentile(values, 95)
        
        if abs(p95 - p05) < 1e-6:  # No variance, fall back to neutral
            return 0.5
        
        # Scale and clip to [0,1]
        scaled = (target_value - p05) / (p95 - p05)
        return np.clip(scaled, 0, 1)
    
    def compute_composite_scores(self, metrics: Dict[str, Any], all_metrics: List[Dict[str, Any]]) -> Dict[str, float]:
        """Compute composite scores using new sign-clean approach with percentile scaling"""
        composites = {}
        
        # Extract alignment values for scaling
        x_user_align_values = [m.get('x_user_align') for m in all_metrics if m.get('x_user_align') is not None]
        x_prior_align_values = [m.get('x_prior_align') for m in all_metrics if m.get('x_prior_align') is not None]
        i_user_align_values = [m.get('i_user_align') for m in all_metrics if m.get('i_user_align') is not None]
        i_prior_align_values = [m.get('i_prior_align') for m in all_metrics if m.get('i_prior_align') is not None]
        x_meta_values = [m.get('x_meta_language_share', 0) / 100.0 for m in all_metrics]  # Convert % to proportion
        
        # External theatre evidence score - unweighted combination of behavior and language
        # x_theatre_evidence_score = x_prior_align_scaled + x_arbitration_rate (equal weighting)
        x_prior_align = metrics.get('x_prior_align', 0)
        x_arbitration_rate = metrics.get('x_arbitration_rate', 0)
        
        # Scale alignment only if we have a reference distribution
        x_prior_align_scaled = None
        if x_prior_align_values:
            x_prior_align_scaled = self.percentile_scale(x_prior_align_values, x_prior_align)
        
        # Compute external theatre evidence with equal weighting (no w_align/w_ctrl)
        acc, components = 0.0, 0
        if x_prior_align_scaled is not None:
            acc += x_prior_align_scaled
            components += 1
        if x_arbitration_rate is not None:
            acc += x_arbitration_rate
            components += 1
        
        if components > 0:
            composites['x_theatre_evidence_score'] = acc / components
        else:
            print("âš ï¸ Warning: No signals available for x_theatre_evidence_score, setting to NaN")
            composites['x_theatre_evidence_score'] = float('nan')
        
        # Meta-without-control score  
        # x_meta_without_control_score = x_meta_language_share_scaled * 1[x_arbitration_rate == 0]
        x_meta_share = metrics.get('x_meta_language_share', 0) / 100.0  # Convert % to proportion
        if x_meta_values:
            x_meta_scaled = self.percentile_scale(x_meta_values, x_meta_share)
            composites['x_meta_without_control_score'] = x_meta_scaled if x_arbitration_rate == 0 else 0.0
        else:
            print("âš ï¸ Warning: No meta-language scaling distribution available, setting x_meta_without_control_score to NaN")
            composites['x_meta_without_control_score'] = float('nan')
        
        # Normalized meta-without-control score: S(x_meta_language_share) * 1[x_arbitration_rate==0] - S(i_meta_language_share)
        # Extract i_meta_language values for scaling
        i_meta_values = [m.get('i_meta_language_share', 0) / 100.0 for m in all_metrics if m.get('i_meta_language_share') is not None]
        i_meta_share = metrics.get('i_meta_language_share', 0) / 100.0  # Convert % to proportion
        
        if x_meta_values and i_meta_values:
            # Surface component: scaled external meta-language share if no surface arbitration, else 0
            surface_component = x_meta_scaled if x_arbitration_rate == 0 else 0.0
            # Internal component: scaled internal meta-language share
            i_meta_scaled = self.percentile_scale(i_meta_values, i_meta_share)
            composites['meta_without_control_norm'] = surface_component - i_meta_scaled
        else:
            composites['meta_without_control_norm'] = float('nan')
        
        # Internal theatre evidence score - handle three states (Missing/Available-but-zero/Available-and-positive)
        
        # detect whether any internal content exists for this trial
        internal_available = bool(
            metrics.get("i_sentence_count", 0) > 0
            or str(metrics.get("ST_after", "")).strip()
        )

        # fetch raw inputs without coercing to 0 (let None mean "missing")
        i_prior_align_raw = metrics.get("i_prior_align")          # None if missing
        i_arbitration_rate = metrics.get("i_arbitration_rate")    # None if missing

        # scale alignment only if we have both a value and a reference distribution
        i_prior_align_scaled = None
        if i_prior_align_raw is not None and i_prior_align_values:
            i_prior_align_scaled = self.percentile_scale(i_prior_align_values, i_prior_align_raw)

        # compute composite with weight re-normalization, or NaN when no internals
        if not internal_available:
            composites["i_theatre_evidence_score"] = float("nan")
        else:
            # if internals exist but arb rate missing, that means "no hits" â‡’ 0
            if i_arbitration_rate is None:
                i_arbitration_rate = 0.0

            # re-normalize weights to whatever signals are present
            # Equal weighting for internal theatre evidence (no w_align/w_ctrl)
            acc, components = 0.0, 0
            if i_prior_align_scaled is not None:
                acc += i_prior_align_scaled
                components += 1
            if i_arbitration_rate is not None:
                acc += i_arbitration_rate
                components += 1

            composites["i_theatre_evidence_score"] = (acc / components) if components > 0 else float("nan")
        
        # Theatre Exposure Index - only compute when both external and internal arbitration data available
        # +1 (internal-only), 0 (aligned), -1 (surface-only)
        if (internal_available and 
            i_arbitration_rate is not None and 
            x_arbitration_rate is not None):
            if x_arbitration_rate == 0 and i_arbitration_rate == 1:
                theatre_exposure = 1  # Hidden theatre
            elif x_arbitration_rate == 1 and i_arbitration_rate == 0:
                theatre_exposure = -1  # Surface-only (rare)
            else:
                theatre_exposure = 0  # Aligned
            composites['theatre_exposure_index'] = theatre_exposure
        else:
            # Cannot determine exposure without both external and internal arbitration data
            composites['theatre_exposure_index'] = float('nan')
        
        # Spillover Index: S(i_option_count_strict) - S(x_option_count_strict)
        # Extract option count values for scaling
        i_option_strict_values = [m.get('i_option_count_strict', 0) for m in all_metrics if m.get('i_option_count_strict') is not None]
        x_option_strict_values = [m.get('x_option_count_strict', 0) for m in all_metrics if m.get('x_option_count_strict') is not None]
        
        i_option_strict = metrics.get('i_option_count_strict', 0)
        x_option_strict = metrics.get('x_option_count_strict', 0)
        
        # Only compute spillover if we have both internal and external option data
        if i_option_strict_values and x_option_strict_values and i_option_strict is not None:
            i_option_scaled = self.percentile_scale(i_option_strict_values, i_option_strict)
            x_option_scaled = self.percentile_scale(x_option_strict_values, x_option_strict)
            composites['spill_index'] = i_option_scaled - x_option_scaled
        else:
            composites['spill_index'] = float('nan')
        
        # EFE-proxy computation using locked formulas
        # Extract x_user_align values for scaling
        x_user_align_values = [m.get('x_user_align') for m in all_metrics if m.get('x_user_align') is not None]
        x_user_align = metrics.get('x_user_align')
        
        # efe_R = 1 - S(x_user_align)
        if x_user_align_values and x_user_align is not None:
            x_user_align_scaled = self.percentile_scale(x_user_align_values, x_user_align)
            composites['efe_R'] = 1.0 - x_user_align_scaled
        else:
            composites['efe_R'] = float('nan')
        
        # efe_E = mean_avail(i_arbitration_rate, min(i_option_count_strict,5)/5, max(0, TEI)) - 0.5*x_arbitration_rate
        i_arbitration_rate = metrics.get('i_arbitration_rate')
        i_option_count_strict = metrics.get('i_option_count_strict', 0)
        theatre_exposure_index = composites.get('theatre_exposure_index')
        x_arbitration_rate = metrics.get('x_arbitration_rate', 0)
        
        # Build components for mean_avail
        e_components = []
        if i_arbitration_rate is not None:
            e_components.append(i_arbitration_rate)
        if i_option_count_strict is not None:
            # Normalize option count to [0,1] by dividing by 5 (the cap)
            i_option_count_norm = min(i_option_count_strict, 5) / 5.0
            e_components.append(i_option_count_norm)
        if theatre_exposure_index is not None and not np.isnan(theatre_exposure_index):
            # max(0, TEI) - TEI can be -1, 0, or +1
            tei_component = max(0, theatre_exposure_index)
            e_components.append(tei_component)
        
        # mean_avail: average only present components; if none present, return 0.0
        if e_components:
            mean_avail_result = sum(e_components) / len(e_components)
        else:
            mean_avail_result = 0.0
            
        # Complete efe_E formula
        surface_penalty = 0.5 * x_arbitration_rate if x_arbitration_rate is not None else 0.0
        composites['efe_E'] = mean_avail_result - surface_penalty
        
        # efe_Ghat = efe_R - efe_E
        if not np.isnan(composites['efe_R']) and not np.isnan(composites['efe_E']):
            composites['efe_Ghat'] = composites['efe_R'] - composites['efe_E']
        else:
            composites['efe_Ghat'] = float('nan')
        
        return composites
    
    def compute_deictic_stats(self, text: str) -> Dict[str, float]:
        """Compute deictic statistics - person/temporal/spatial deixis rates"""
        if not text.strip():
            return {"fp_rate": 0.0, "sp_rate": 0.0, "temp_rate": 0.0, "spat_rate": 0.0}
        
        # Normalize curly quotes to straight quotes
        normalized_text = text.replace('\u2019', "'").replace('\u201c', '"').replace('\u201d', '"')
        
        # Strip headers like SYSTEM-THOUGHT: / SYSTEM-OUTPUT:
        cleaned_text = re.sub(r'^SYSTEM-(THOUGHT|OUTPUT):\s*', '', normalized_text, flags=re.IGNORECASE)
        
        # Tokenize using existing spaCy approach
        doc = self.nlp(cleaned_text)
        tokens = [t for t in doc if not t.is_space]
        total_tokens = len(tokens)
        
        if total_tokens == 0:
            return {"fp_rate": 0.0, "sp_rate": 0.0, "temp_rate": 0.0, "spat_rate": 0.0}
        
        # Define deictic patterns with word boundaries, case-insensitive
        fp_pattern = re.compile(r'\b(I|me|my|mine|we|us|our|ours)\b', re.IGNORECASE)
        sp_pattern = re.compile(r'\b(you|your|yours|you\'re)\b', re.IGNORECASE) 
        temp_pattern = re.compile(r'\b(now|today|yesterday|tomorrow|currently|then|soon|immediately)\b', re.IGNORECASE)
        spat_pattern = re.compile(r'\b(here|there|this|that|these|those)\b', re.IGNORECASE)
        
        # Count matches with guardrails
        fp_count = 0
        
        # Process first-person pronouns with guardrails
        for match in fp_pattern.finditer(cleaned_text):
            matched_word = match.group()
            
            # Skip "US" (country) when all caps or dotted variants
            if matched_word.lower() == "us" and (matched_word.isupper() or re.match(r"U\.S\.A?\.?$", matched_word)):
                continue
                
            # Skip "I" in list/roman numeral contexts: "I." or "(I)"
            if matched_word.upper() == "I":
                start_pos = match.start()
                end_pos = match.end()
                if end_pos < len(cleaned_text) and cleaned_text[end_pos] == '.':
                    continue
                if (start_pos > 0 and cleaned_text[start_pos-1] == '(') and (end_pos < len(cleaned_text) and cleaned_text[end_pos] == ')'):
                    continue
            
            fp_count += 1
        
        # Count other deictic types
        sp_count = len(sp_pattern.findall(cleaned_text))
        temp_count = len(temp_pattern.findall(cleaned_text))
        spat_count = len(spat_pattern.findall(cleaned_text))
        
        return {
            "fp_rate": fp_count / total_tokens,
            "sp_rate": sp_count / total_tokens,
            "temp_rate": temp_count / total_tokens,
            "spat_rate": spat_count / total_tokens
        }
    
    def compute_metrics(self, trial_data: Dict[str, Any]) -> Dict[str, Any]:
        """Compute all metrics for a single trial using new naming convention"""
        metrics = {}
        
        # Extract data
        r1 = trial_data.get('R1', '')
        r2 = trial_data.get('R2', '')
        u1 = trial_data.get('U1', '')
        u2 = trial_data.get('U2', '')
        st_after = trial_data.get('ST_after', '')
        topic = trial_data.get('topic', '')
        
        # Log token counts for verbosity bias analysis
        if r2.strip():
            r2_doc = self.nlp(r2)
            metrics['token_count_R2'] = len([t for t in r2_doc if not t.is_space])
        else:
            metrics['token_count_R2'] = 0
            
        if st_after.strip():
            st_doc = self.nlp(st_after)
            metrics['token_count_ST'] = len([t for t in st_doc if not t.is_space])
            metrics['i_sentence_count'] = len(list(st_doc.sents))
        else:
            metrics['token_count_ST'] = 0
            metrics['i_sentence_count'] = 0
        
        # External/surface metrics (computed on R2)
        metrics['x_topic_focus_share'] = self.compute_x_topic_focus_share(r2, topic)
        metrics['x_meta_language_share'] = self.compute_x_meta_language_share(r2)
        metrics['x_arbitration_rate_strict'] = self.compute_x_arbitration_rate_strict(r2)
        metrics['x_arbitration_rate_lenient'] = self.compute_x_arbitration_rate_lenient(r2)
        # Use default arbitration pattern (strict union) instead of max of strict/lenient
        metrics['x_arbitration_rate'] = 1 if self.pattern_manager.match_category(r2, 'arbitration', 'default') else 0
        metrics['x_topic_obedience_rate'] = self.compute_x_topic_obedience_rate(r2, topic)
        metrics['x_topic_integration_share'] = self.compute_x_topic_integration_share(r2, r1, topic)
        metrics['x_option_count_strict'] = self.compute_x_option_count_strict(r2, topic, r1)
        metrics['x_option_count_lenient'] = self.compute_x_option_count_lenient(r2, topic, r1)
        
        # Compute hedge/certainty shares for R2 (external)
        x_hedge_cert = self.compute_hedge_certainty_shares(r2)
        metrics['x_hedge_share'] = x_hedge_cert['hedge_share']
        metrics['x_cert_share'] = x_hedge_cert['cert_share']
        
        # Compute perspective obedience and role confusion for R2 (external)
        x_perspective = self.compute_perspective_obedience(r2)
        metrics['x_perspective_obedience'] = x_perspective['x_perspective_obedience']
        metrics['x_role_confusion_hits'] = x_perspective['x_role_confusion_hits']
        
        # Compute deictic statistics for R2 (external)
        x_deictic_stats = self.compute_deictic_stats(r2)
        metrics['x_fp_rate'] = x_deictic_stats['fp_rate']
        metrics['x_sp_rate'] = x_deictic_stats['sp_rate'] 
        metrics['x_temp_rate'] = x_deictic_stats['temp_rate']
        metrics['x_spat_rate'] = x_deictic_stats['spat_rate']
        
        # Addressivity metrics (surface)
        total_deictic = x_deictic_stats['sp_rate'] + x_deictic_stats['fp_rate']
        if total_deictic < 5e-4:  # Deictic-sparse text
            metrics['x_addr_share'] = np.nan
            metrics['x_addr_ratio_raw'] = np.nan  # Diagnostic only
        else:
            metrics['x_addr_share'] = x_deictic_stats['sp_rate'] / (x_deictic_stats['sp_rate'] + x_deictic_stats['fp_rate'] + 1e-6)
            metrics['x_addr_ratio_raw'] = x_deictic_stats['sp_rate'] / (x_deictic_stats['fp_rate'] + 1e-6)  # Diagnostic only
        
        # Compute dual alignment scores for R2
        x_user_align, x_prior_align = self.compute_alignment_scores(r2, r1, u2)
        metrics['x_user_align'] = x_user_align      # +ve = follows user
        metrics['x_prior_align'] = x_prior_align    # +ve = follows prior
        
        # Internal metrics (computed on ST_after, if available)
        if st_after.strip():
            metrics['i_arbitration_rate_strict'] = self.compute_i_arbitration_rate_strict(st_after)
            metrics['i_arbitration_rate_lenient'] = self.compute_i_arbitration_rate_lenient(st_after)
            # Use default arbitration pattern for internal thoughts too
            metrics['i_arbitration_rate'] = 1 if self.pattern_manager.match_category(st_after, 'arbitration', 'default') else 0
            metrics['i_option_count_strict'] = self.compute_i_option_count_strict(st_after, topic, r1)
            metrics['i_option_count_lenient'] = self.compute_i_option_count_lenient(st_after, topic, r1)
            metrics['i_option_count'] = metrics['i_option_count_strict'] + metrics['i_option_count_lenient']  # Sum both counts
            metrics['i_topic_integration_share'] = self.compute_i_topic_integration_share(st_after, r1, topic)
            metrics['i_meta_language_share'] = self.compute_i_meta_language_share(st_after)
            
            # Compute dual alignment scores for ST_after
            i_user_align, i_prior_align = self.compute_alignment_scores(st_after, r1, u2)
            metrics['i_user_align'] = i_user_align      # +ve = follows user
            metrics['i_prior_align'] = i_prior_align    # +ve = follows prior
            
            # Compute deictic statistics for ST_after (internal)
            i_deictic_stats = self.compute_deictic_stats(st_after)
            metrics['i_fp_rate'] = i_deictic_stats['fp_rate']
            metrics['i_sp_rate'] = i_deictic_stats['sp_rate']
            metrics['i_temp_rate'] = i_deictic_stats['temp_rate']
            metrics['i_spat_rate'] = i_deictic_stats['spat_rate']
            
            # Deictic shift (internal â†’ surface)
            metrics['delta_fp'] = metrics['x_fp_rate'] - i_deictic_stats['fp_rate']
            metrics['delta_sp'] = metrics['x_sp_rate'] - i_deictic_stats['sp_rate']  
            metrics['delta_temp'] = metrics['x_temp_rate'] - i_deictic_stats['temp_rate']
            metrics['delta_spat'] = metrics['x_spat_rate'] - i_deictic_stats['spat_rate']
            
            # Compute hedge/certainty shares for ST_after (internal)
            i_hedge_cert = self.compute_hedge_certainty_shares(st_after)
            metrics['i_hedge_share'] = i_hedge_cert['hedge_share']
            metrics['i_cert_share'] = i_hedge_cert['cert_share']
        else:
            # No internal data available - set internal metrics to NaN and deltas to NaN
            metrics['i_fp_rate'] = float('nan')
            metrics['i_sp_rate'] = float('nan')
            metrics['i_temp_rate'] = float('nan')
            metrics['i_spat_rate'] = float('nan')
            metrics['delta_fp'] = float('nan')
            metrics['delta_sp'] = float('nan')
            metrics['delta_temp'] = float('nan')
            metrics['delta_spat'] = float('nan')
            metrics['i_hedge_share'] = float('nan')
            metrics['i_cert_share'] = float('nan')
            metrics['i_meta_language_share'] = float('nan')
        
        # Compression ratio and style distance
        if st_after.strip():
            # Internal thoughts available - compute compression ratio
            metrics['comp_ratio'] = metrics['token_count_R2'] / max(metrics['token_count_ST'], 1)
            
            # Style distance (cosine distance between R2 and ST_after embeddings)
            try:
                if r2.strip():
                    # Preprocess both texts identically (strip headers)
                    r2_clean = re.sub(r'^SYSTEM-(THOUGHT|OUTPUT):\s*', '', r2, flags=re.IGNORECASE)
                    st_clean = re.sub(r'^SYSTEM-(THOUGHT|OUTPUT):\s*', '', st_after, flags=re.IGNORECASE)
                    
                    # Generate embeddings using the same model as alignment calculations
                    r2_embedding = self.embedder.encode([r2_clean])
                    st_embedding = self.embedder.encode([st_clean])
                    
                    # Compute cosine similarity and convert to distance
                    cosine_sim = cosine_similarity(r2_embedding, st_embedding)[0, 0]
                    metrics['style_dist'] = 1.0 - cosine_sim
                else:
                    metrics['style_dist'] = float('nan')
            except Exception:
                # If embeddings fail for any reason, set to NaN
                metrics['style_dist'] = float('nan')
        else:
            # No internal thoughts available (e.g., scenario E)
            metrics['comp_ratio'] = float('nan')
            metrics['style_dist'] = float('nan')
        
        # Derived hedge/certainty metrics
        # Equanimity style index (surface): eq_style = hedge_share(R2) - cert_share(R2)
        metrics['eq_style'] = metrics['x_hedge_share'] - metrics['x_cert_share']
        
        # Internalâ†’surface hedge shift: delta_hedge = hedge_share(R2) - hedge_share(ST_after)
        if not np.isnan(metrics['i_hedge_share']):
            metrics['delta_hedge'] = metrics['x_hedge_share'] - metrics['i_hedge_share']
        else:
            metrics['delta_hedge'] = float('nan')
        
        return metrics
    
    def apply_exclusions(self, trials: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:
        """Apply exclusion criteria and return filtered trials with exclusion stats"""
        exclusions = {'topic_leakage': 0, 'short_r1': 0, 'total': len(trials)}
        filtered_trials = []
        
        for trial in trials:
            r1 = trial.get('R1', '')
            topic = trial.get('topic', '')
            
            # Check for topic leakage in R1
            if topic:
                topic_pattern = self.get_topic_lemma_pattern(topic)
                if re.search(topic_pattern, r1, re.IGNORECASE):
                    exclusions['topic_leakage'] += 1
                    continue
            
            # Check R1 length (â‰¥20 tokens)
            if r1.strip():
                doc = self.nlp(r1)
                token_count = len([t for t in doc if not t.is_space])
                if token_count < 20:
                    exclusions['short_r1'] += 1
                    continue
            else:
                exclusions['short_r1'] += 1
                continue
            
            filtered_trials.append(trial)
        
        exclusions['retained'] = len(filtered_trials)
        return filtered_trials, exclusions
    
    def bootstrap_ci(self, data: List[float], confidence: float = 0.95, n_bootstrap: int = 2000) -> Tuple[float, float, float]:
        """Compute bootstrap confidence interval"""
        if not data:
            return np.nan, np.nan, np.nan
            
        data = [x for x in data if x is not None and not np.isnan(x)]
        if not data:
            return np.nan, np.nan, np.nan
            
        # Original statistic
        original_stat = np.mean(data)
        
        # Bootstrap sampling
        bootstrap_stats = []
        for _ in range(n_bootstrap):
            bootstrap_sample = np.random.choice(data, size=len(data), replace=True)
            bootstrap_stats.append(np.mean(bootstrap_sample))
        
        # Compute confidence interval
        alpha = 1 - confidence
        lower_percentile = (alpha / 2) * 100
        upper_percentile = (1 - alpha / 2) * 100
        
        ci_lower = np.percentile(bootstrap_stats, lower_percentile)
        ci_upper = np.percentile(bootstrap_stats, upper_percentile)
        
        return original_stat, ci_lower, ci_upper
    
    def bootstrap_contrast(self, df: pd.DataFrame, metric: str, scenario_a: str, scenario_b: str, 
                          strata: List[str] = ['topic', 'framing'], n_boot: int = 2000, seed: int = 2025) -> Dict[str, Any]:
        """Compute stratified bootstrap contrast between two scenarios with robust NaN handling"""
        np.random.seed(seed)
        
        # Filter to scenarios (create explicit copies to avoid pandas warnings)
        df_a = df[df['scenario'] == scenario_a].copy()
        df_b = df[df['scenario'] == scenario_b].copy()
        
        if df_a.empty or df_b.empty:
            reason = f"empty_data: scenario_a({scenario_a})={len(df_a)}, scenario_b({scenario_b})={len(df_b)}"
            return {'delta': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan, 'bootstrap_directional_support': np.nan, 'nan_reason': reason}
        
        # Check for valid (non-NaN) data in both scenarios
        valid_a = df_a[metric].dropna()
        valid_b = df_b[metric].dropna()
        
        if len(valid_a) == 0:
            # More specific messaging for internal vs external metrics
            if metric.startswith('i_'):
                reason = f"no_i_data_in_reference: {scenario_a} has no internal data for {metric}"
            else:
                reason = f"no_x_data_in_reference: {scenario_a} has no external data for {metric}"
            return {'delta': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan, 'bootstrap_directional_support': np.nan, 'nan_reason': reason}
        
        if len(valid_b) == 0:
            # More specific messaging for internal vs external metrics  
            if metric.startswith('i_'):
                reason = f"no_i_data_in_comparison: {scenario_b} has no internal data for {metric}"
            else:
                reason = f"no_x_data_in_comparison: {scenario_b} has no external data for {metric}"
            return {'delta': np.nan, 'ci_lower': np.nan, 'ci_upper': np.nan, 'bootstrap_directional_support': np.nan, 'nan_reason': reason}
        
        # Create valid data dataframes for all cases (needed for bootstrap sampling)
        df_a_valid = df_a[df_a[metric].notna()].copy()
        df_b_valid = df_b[df_b[metric].notna()].copy()
        
        # Get unique strata combinations using valid data only
        if strata:
            # Create stratum identifier for valid data
            df_a_valid['stratum'] = df_a_valid[strata].apply(lambda x: '_'.join(x.astype(str)), axis=1)
            df_b_valid['stratum'] = df_b_valid[strata].apply(lambda x: '_'.join(x.astype(str)), axis=1)
            
            # Only use strata that exist in both scenarios
            common_strata = set(df_a_valid['stratum']) & set(df_b_valid['stratum'])
            if not common_strata:
                strata = []  # Fall back to unstratified
        
        # Original difference using valid data only
        original_delta = valid_a.mean() - valid_b.mean()
        
        # Bootstrap resampling using only valid data
        bootstrap_deltas = []
        for _ in range(n_boot):
            if strata and common_strata:
                # Stratified resampling with valid data
                boot_a_values = []
                boot_b_values = []
                
                for stratum in common_strata:
                    # Resample within each stratum using valid data only
                    a_stratum = df_a_valid[df_a_valid['stratum'] == stratum][metric].dropna().values
                    b_stratum = df_b_valid[df_b_valid['stratum'] == stratum][metric].dropna().values
                    
                    if len(a_stratum) > 0 and len(b_stratum) > 0:
                        boot_a_values.extend(np.random.choice(a_stratum, size=len(a_stratum), replace=True))
                        boot_b_values.extend(np.random.choice(b_stratum, size=len(b_stratum), replace=True))
                
                if boot_a_values and boot_b_values:
                    boot_delta = np.mean(boot_a_values) - np.mean(boot_b_values)
                else:
                    boot_delta = np.nan
            else:
                # Unstratified resampling using valid data only
                boot_a = np.random.choice(valid_a.values, size=len(valid_a), replace=True)
                boot_b = np.random.choice(valid_b.values, size=len(valid_b), replace=True)
                boot_delta = np.mean(boot_a) - np.mean(boot_b)
            
            if not np.isnan(boot_delta):
                bootstrap_deltas.append(boot_delta)
        
        if not bootstrap_deltas:
            reason = f"bootstrap_failed: no valid bootstrap deltas generated from {n_boot} iterations"
            return {'delta': original_delta, 'ci_lower': np.nan, 'ci_upper': np.nan, 'bootstrap_directional_support': np.nan, 'nan_reason': reason}
        
        # Compute CI and two-sided bootstrap p-value
        ci_lower = np.percentile(bootstrap_deltas, 2.5)
        ci_upper = np.percentile(bootstrap_deltas, 97.5)
        
        # Bootstrap directional support (probability that bootstrap samples support observed effect direction)
        if original_delta == 0:
            bootstrap_directional_support = 1.0
        else:
            bootstrap_directional_support = np.mean([abs(d) >= abs(original_delta) for d in bootstrap_deltas])
        
        return {
            'delta': original_delta,
            'ci_lower': ci_lower,
            'ci_upper': ci_upper,
            'bootstrap_directional_support': bootstrap_directional_support,  # Directional evidence, not p-value
            'n_a': len(valid_a),  # Count of valid data points, not total rows
            'n_b': len(valid_b),  # Count of valid data points, not total rows
            'nan_reason': None  # Successful contrast, no NaN reason
        }

    def resolve_references_per_stratum(self, df: pd.DataFrame, reference_scenario_x: Optional[str] = None, reference_scenario_i: Optional[str] = None) -> Dict[str, Any]:
        """Resolve dual references per (topic, model) stratum for methodologically sound comparisons"""
        scenarios = df['scenario'].unique()
        strata_references = {}
        
        # Group scenarios by (topic, model) combinations
        strata = {}
        for scenario in scenarios:
            letter, topic, model = self.parse_scenario_name(scenario)
            stratum = f"{topic}_{model}"
            if stratum not in strata:
                strata[stratum] = []
            strata[stratum].append(scenario)
        
        print(f"ðŸ“Š Resolving dual references for {len(strata)} strata: {list(strata.keys())}")
        
        # Determine data availability for each scenario
        scenario_data_availability = {}
        for scenario in scenarios:
            scenario_df = df[df['scenario'] == scenario]
            
            # Check external data availability (any non-NaN external metrics)
            has_external = False
            external_metrics = ['x_topic_focus_share', 'x_arbitration_rate', 'x_user_align', 'x_fp_rate']
            for metric in external_metrics:
                if metric in df.columns:
                    values = scenario_df[metric].dropna()
                    if len(values) > 0:  # Any valid values indicate external data available
                        has_external = True
                        break
            
            # Check internal data availability (primary: i_sentence_count, fallback: any i_* metrics)
            has_internal = False
            if 'i_sentence_count' in df.columns:
                internal_counts = scenario_df['i_sentence_count'].fillna(0)
                has_internal = (internal_counts > 0).any()
            else:
                # Fallback: check if any internal metrics have non-NaN values
                internal_metrics = ['i_arbitration_rate', 'i_user_align', 'i_prior_align', 'i_fp_rate']
                for metric in internal_metrics:
                    if metric in df.columns:
                        values = scenario_df[metric].dropna()
                        if len(values) > 0:
                            has_internal = True
                            break
            
            scenario_data_availability[scenario] = {
                'has_external': has_external,
                'has_internal': has_internal
            }
        
        # Resolve references for each stratum
        for stratum, scenario_list in strata.items():
            topic, model = stratum.split('_', 1)
            
            # Find reference_scenario_x (external metrics - any scenario with external data)
            ref_x_candidates = [s for s in scenario_list if scenario_data_availability[s]['has_external']]
            if reference_scenario_x and reference_scenario_x in ref_x_candidates:
                chosen_ref_x = reference_scenario_x
            elif reference_scenario_x:
                # Pattern match
                pattern_matches = [s for s in ref_x_candidates if s.startswith(reference_scenario_x)]
                chosen_ref_x = pattern_matches[0] if pattern_matches else (ref_x_candidates[0] if ref_x_candidates else None)
            else:
                chosen_ref_x = sorted(ref_x_candidates)[0] if ref_x_candidates else None
            
            # Find reference_scenario_i (internal metrics - scenario with internal data, preferably not E)
            ref_i_candidates = [s for s in scenario_list if scenario_data_availability[s]['has_internal']]
            non_e_candidates = [s for s in ref_i_candidates if not self.parse_scenario_name(s)[0].startswith('E')]
            
            if reference_scenario_i and reference_scenario_i in ref_i_candidates:
                chosen_ref_i = reference_scenario_i
            elif reference_scenario_i:
                # Pattern match
                pattern_matches = [s for s in ref_i_candidates if s.startswith(reference_scenario_i)]
                chosen_ref_i = pattern_matches[0] if pattern_matches else (ref_i_candidates[0] if ref_i_candidates else None)
            else:
                # Prefer non-E scenarios for internal reference, fallback to any with internal data
                chosen_ref_i = (sorted(non_e_candidates)[0] if non_e_candidates else 
                              (sorted(ref_i_candidates)[0] if ref_i_candidates else None))
            
            strata_references[stratum] = {
                'topic': topic,
                'model': model, 
                'scenarios': scenario_list,
                'reference_scenario_x': chosen_ref_x,
                'reference_scenario_i': chosen_ref_i,
                'external_available': ref_x_candidates,
                'internal_available': ref_i_candidates
            }
            
            # Provide actionable warnings for missing references
            if not chosen_ref_x and ref_x_candidates:
                print(f"âš ï¸  No external reference found for {stratum} despite available candidates: {ref_x_candidates}")
            elif not chosen_ref_x:
                print(f"âš ï¸  No external data available in any scenario for {stratum}")
                
            if not chosen_ref_i and ref_i_candidates:
                print(f"âš ï¸  No internal reference found for {stratum} despite available candidates: {ref_i_candidates}")
            elif not chosen_ref_i:
                print(f"âš ï¸  No internal data available in any scenario for {stratum}")
                
            print(f"  {stratum}: ref_x={chosen_ref_x}, ref_i={chosen_ref_i}")
        
        return {
            'strata_references': strata_references,
            'scenario_data_availability': scenario_data_availability
        }

    def statistical_analysis(self, trials_with_metrics: List[Dict[str, Any]], reference_scenario: Optional[str] = None, reference_scenario_x: Optional[str] = None, reference_scenario_i: Optional[str] = None) -> Dict[str, Any]:
        """Perform scenario-agnostic statistical analysis with dual reference system"""
        print("ðŸ“Š Performing scenario-agnostic statistical analysis with dual references...")
        
        # Convert to DataFrame for easier analysis
        df = pd.DataFrame(trials_with_metrics)
        
        # Support backward compatibility - if only reference_scenario provided, use for both
        if reference_scenario and not reference_scenario_x and not reference_scenario_i:
            reference_scenario_x = reference_scenario
            reference_scenario_i = reference_scenario
        
        results = {
            'tier_a_contrasts': {},
            'tier_b_models': {},
            'scenario_summaries': {},
            'discovered_contrasts': [],
            'dual_reference_info': None  # Will store reference resolution results
        }
        
        # Get unique scenarios
        scenarios = df['scenario'].unique()
        print(f"ðŸ“‹ Found scenarios: {list(scenarios)}")
        
        # Group trials by scenario
        scenario_groups = {}
        for scenario in scenarios:
            scenario_groups[scenario] = df[df['scenario'] == scenario]
        
        # Tier A: Scenario summaries with bootstrap CIs  
        metric_names = [
            # External/surface metrics
            'x_topic_focus_share', 'x_meta_language_share', 'x_arbitration_rate_strict', 'x_arbitration_rate_lenient', 'x_arbitration_rate',
            'x_user_align', 'x_prior_align', 'x_topic_obedience_rate', 'x_topic_integration_share',
            # External option metrics
            'x_option_count_strict', 'x_option_count_lenient',
            # MPE External metrics - Deictic
            'x_fp_rate', 'x_sp_rate', 'x_temp_rate', 'x_spat_rate', 'x_addr_share',
            # MPE External metrics - Hedge/Certainty
            'x_hedge_share', 'x_cert_share', 'x_perspective_obedience', 'x_role_confusion_hits',
            # Internal metrics
            'i_arbitration_rate_strict', 'i_arbitration_rate_lenient', 'i_arbitration_rate',
            'i_option_count_strict', 'i_option_count_lenient', 'i_option_count',
            'i_user_align', 'i_prior_align', 'i_topic_integration_share', 'i_meta_language_share',
            # MPE Internal metrics - Deictic
            'i_fp_rate', 'i_sp_rate', 'i_temp_rate', 'i_spat_rate',
            # MPE Internal metrics - Hedge/Certainty
            'i_hedge_share', 'i_cert_share',
            # MPE Delta metrics
            'delta_fp', 'delta_sp', 'delta_temp', 'delta_spat', 'delta_hedge',
            # MPE Compression & Style
            'comp_ratio', 'style_dist', 'eq_style',
            # Original composite scores
            'x_theatre_evidence_score', 'x_meta_without_control_score', 'i_theatre_evidence_score', 'theatre_exposure_index',
            # MPE Composite metrics
            'spill_index', 'meta_without_control_norm', 'efe_R', 'efe_E', 'efe_Ghat'
        ]
        
        # Extract metrics into DataFrame columns
        for metric in metric_names:
            df[metric] = df.apply(lambda row: row['computed_metrics'].get(metric) if 'computed_metrics' in row else np.nan, axis=1)
        
        # Compute scenario summaries
        for scenario in scenarios:
            scenario_df = df[df['scenario'] == scenario]
            results['scenario_summaries'][scenario] = {}
            
            for metric in metric_names:
                values = scenario_df[metric].dropna().values
                if len(values) > 0:
                    mean_val, ci_lower, ci_upper = self.bootstrap_ci(values.tolist())
                    results['scenario_summaries'][scenario][metric] = {
                        'mean': mean_val,
                        'ci_lower': ci_lower,
                        'ci_upper': ci_upper,
                        'n': len(values)
                    }
        
        # Resolve dual references per stratum
        dual_ref_info = self.resolve_references_per_stratum(df, reference_scenario_x, reference_scenario_i)
        results['dual_reference_info'] = dual_ref_info
        
        # Tier A: Dual reference-based contrasts  
        external_metrics = [
            'x_user_align', 'x_prior_align', 'x_arbitration_rate', 'x_theatre_evidence_score',
            'x_topic_focus_share', 'x_meta_language_share', 'x_topic_obedience_rate', 'x_topic_integration_share',
            'x_option_count_strict', 'x_option_count_lenient', 'x_meta_without_control_score',
            # MPE External metrics
            'x_fp_rate', 'x_sp_rate', 'x_temp_rate', 'x_spat_rate', 'x_addr_share',
            'x_hedge_share', 'x_cert_share', 'x_perspective_obedience', 'x_role_confusion_hits',
            'eq_style', 'efe_R'
        ]
        internal_metrics = [
            'i_user_align', 'i_prior_align', 'i_arbitration_rate', 'i_theatre_evidence_score',
            'i_option_count_strict', 'i_option_count_lenient', 'i_topic_integration_share', 'i_meta_language_share',
            # MPE Internal metrics  
            'i_fp_rate', 'i_sp_rate', 'i_temp_rate', 'i_spat_rate',
            'i_hedge_share', 'i_cert_share',
            # MPE Delta and composite metrics
            'delta_fp', 'delta_sp', 'delta_temp', 'delta_spat', 'delta_hedge',
            'comp_ratio', 'style_dist', 'spill_index', 'meta_without_control_norm', 'efe_E', 'efe_Ghat'
        ]
        all_key_metrics = external_metrics + internal_metrics
        
        # Generate contrasts using appropriate references per stratum
        for stratum, stratum_info in dual_ref_info['strata_references'].items():
            ref_scenario_x = stratum_info['reference_scenario_x']
            ref_scenario_i = stratum_info['reference_scenario_i']
            stratum_scenarios = stratum_info['scenarios']
            
            print(f"ðŸ“Š Processing stratum {stratum}: external_ref={ref_scenario_x}, internal_ref={ref_scenario_i}")
            
            # Handle case where no valid references exist for this stratum
            if not ref_scenario_x and not ref_scenario_i:
                print(f"âš ï¸  No valid references found for stratum {stratum} - will show meanÂ±CI instead of contrasts")
                # Create a summary entry for this stratum showing individual scenario means
                for scenario in stratum_scenarios:
                    if scenario in results['scenario_summaries']:
                        summary_name = f"summary_{scenario}_no_ref"
                        results['tier_a_contrasts'][summary_name] = {
                            'description': f"{scenario} summary (no reference available)",
                            'metrics': {},
                            'stratum': stratum,
                            'reference_type': 'none_available'
                        }
                        # Copy scenario summary as pseudo-contrast for consistent reporting
                        for metric in external_metrics + internal_metrics:
                            if metric in results['scenario_summaries'][scenario]:
                                summary_data = results['scenario_summaries'][scenario][metric]
                                results['tier_a_contrasts'][summary_name]['metrics'][metric] = {
                                    'delta': np.nan,
                                    'ci_lower': summary_data['ci_lower'],
                                    'ci_upper': summary_data['ci_upper'], 
                                    'bootstrap_directional_support': np.nan,
                                    'nan_reason': f'no_reference_for_stratum: no valid reference scenarios in {stratum}',
                                    'n_a': summary_data['n'],
                                    'n_b': 0,
                                    'mean': summary_data['mean']  # Store mean for special display
                                }
                continue  # Skip contrast generation for this stratum
            
            # Generate contrasts for each scenario in this stratum  
            for other_scenario in stratum_scenarios:
                other_letter, other_topic, other_model = self.parse_scenario_name(other_scenario)
                
                # External metric contrasts (using reference_scenario_x)
                if ref_scenario_x and other_scenario != ref_scenario_x:
                    ref_letter, _, _ = self.parse_scenario_name(ref_scenario_x)
                    if ref_letter != other_letter:  # Different experimental conditions
                        contrast_name = f"{ref_scenario_x}_vs_{other_scenario}"
                        results['tier_a_contrasts'][contrast_name] = {
                            'description': f"{ref_scenario_x} vs {other_scenario} (external metrics)",
                            'metrics': {},
                            'stratum': stratum,
                            'reference_type': 'external'
                        }
                        
                        for metric in external_metrics:
                            if metric in df.columns:
                                contrast_result = self.bootstrap_contrast(df, metric, ref_scenario_x, other_scenario)
                                results['tier_a_contrasts'][contrast_name]['metrics'][metric] = contrast_result
                                
                                # Log significant findings
                                delta = contrast_result.get('delta')
                                ci_lower = contrast_result.get('ci_lower')
                                ci_upper = contrast_result.get('ci_upper')
                                nan_reason = contrast_result.get('nan_reason')
                                
                                if nan_reason:
                                    print(f"ðŸ“Š {contrast_name} {metric}: NaN ({nan_reason})")
                                elif not np.isnan(delta):
                                    if ci_lower > 0:
                                        sig_status = "âœ… SIGNIFICANT POSITIVE"
                                    elif ci_upper < 0:
                                        sig_status = "âœ… SIGNIFICANT NEGATIVE"  
                                    elif abs(delta) > 0.1:
                                        sig_status = "âš ï¸ MODERATE EFFECT"
                                    else:
                                        sig_status = ""
                                    
                                    if sig_status:
                                        print(f"ðŸ“Š {contrast_name} {metric}: Î”={delta:.3f} [{ci_lower:.3f}, {ci_upper:.3f}] {sig_status}")

                # Internal metric contrasts (using reference_scenario_i)
                if other_scenario != ref_scenario_i:  # Attempt internal contrasts even if ref_scenario_i is None
                    if not ref_scenario_i:
                        # No internal reference available - create NaN contrasts with clear reason
                        contrast_name_i = f"no_internal_ref_vs_{other_scenario}"
                        results['tier_a_contrasts'][contrast_name_i] = {
                            'description': f"No internal reference vs {other_scenario} (internal metrics)",
                            'metrics': {},
                            'stratum': stratum,
                            'reference_type': 'internal_unavailable'
                        }
                        
                        for metric in internal_metrics:
                            if metric in df.columns:
                                results['tier_a_contrasts'][contrast_name_i]['metrics'][metric] = {
                                    'delta': np.nan,
                                    'ci_lower': np.nan,
                                    'ci_upper': np.nan,
                                    'bootstrap_directional_support': np.nan,
                                    'nan_reason': f'no_internal_reference_in_stratum: no scenarios with internal data in {stratum}',
                                    'n_a': 0,
                                    'n_b': 0
                                }
                    elif ref_scenario_i:
                        ref_letter_i, _, _ = self.parse_scenario_name(ref_scenario_i)
                        if ref_letter_i != other_letter:  # Different experimental conditions
                            contrast_name_i = f"{ref_scenario_i}_vs_{other_scenario}"
                            # Avoid duplicate contrast names by adding suffix if needed
                            if contrast_name_i in results['tier_a_contrasts']:
                                contrast_name_i = f"{ref_scenario_i}_vs_{other_scenario}_internal"
                            
                            results['tier_a_contrasts'][contrast_name_i] = {
                                'description': f"{ref_scenario_i} vs {other_scenario} (internal metrics)",
                                'metrics': {},
                                'stratum': stratum,
                                'reference_type': 'internal'
                            }
                            
                            for metric in internal_metrics:
                                if metric in df.columns:
                                    contrast_result = self.bootstrap_contrast(df, metric, ref_scenario_i, other_scenario)
                                    results['tier_a_contrasts'][contrast_name_i]['metrics'][metric] = contrast_result
                                
                                # Log significant findings
                                delta = contrast_result.get('delta')
                                ci_lower = contrast_result.get('ci_lower')
                                ci_upper = contrast_result.get('ci_upper')
                                nan_reason = contrast_result.get('nan_reason')
                                
                                if nan_reason:
                                    print(f"ðŸ“Š {contrast_name_i} {metric}: NaN ({nan_reason})")
                                elif not np.isnan(delta):
                                    if ci_lower > 0:
                                        sig_status = "âœ… SIGNIFICANT POSITIVE"
                                    elif ci_upper < 0:
                                        sig_status = "âœ… SIGNIFICANT NEGATIVE"  
                                    elif abs(delta) > 0.1:
                                        sig_status = "âš ï¸ MODERATE EFFECT"
                                    else:
                                        sig_status = ""
                                    
                                    if sig_status:
                                        print(f"ðŸ“Š {contrast_name_i} {metric}: Î”={delta:.3f} [{ci_lower:.3f}, {ci_upper:.3f}] {sig_status}")
        
        # Store discovered contrasts from dual reference system
        for contrast_name, contrast_data in results['tier_a_contrasts'].items():
            parts = contrast_name.split('_vs_')
            if len(parts) == 2:
                scenario_a = parts[0]
                scenario_b = parts[1]
                
                # Remove "_internal" suffix if present
                if scenario_b.endswith('_internal'):
                    scenario_b = scenario_b[:-9]  # Remove "_internal"
                
                results['discovered_contrasts'].append({
                    'contrast': contrast_name,
                    'scenario_a': scenario_a,
                    'scenario_b': scenario_b,
                    'description': contrast_data['description'],
                    'reference_type': contrast_data.get('reference_type', 'unknown'),
                    'stratum': contrast_data.get('stratum', 'unknown')
                })
        
        return results, df
    
    def copy_enriched_data(self, trials_with_metrics: List[Dict[str, Any]]):
        """Copy original JSON files to output/data/ with computed metrics"""
        print("ðŸ“‹ Copying enriched data to output directory...")
        
        for trial in trials_with_metrics:
            source_dir = trial.get('source_dir', 'unknown')
            source_subdir = trial.get('source_subdir', '')
            
            # Create destination path
            dest_dir = self.output_dir / "data" / Path(source_dir).name / source_subdir
            dest_dir.mkdir(parents=True, exist_ok=True)
            dest_file = dest_dir / "theatre_results.json"
            
            # Create enriched data (remove internal tracking fields)
            enriched_data = trial.copy()
            enriched_data.pop('source_dir', None)
            enriched_data.pop('source_subdir', None) 
            enriched_data.pop('source_path', None)
            
            # Remove duplicate metrics and use computed values only
            enriched_data.pop('metrics', None)  # Remove original empty metrics
            enriched_data['metrics'] = trial.get('computed_metrics', {})
            
            # Save enriched file
            with open(dest_file, 'w') as f:
                json.dump(enriched_data, f, indent=2, default=self._json_serialize)
        
        print(f"âœ… Copied {len(trials_with_metrics)} enriched files to data/")
    
    def _add_pattern_diagnostics(self, findings_text: List[str], scenario: str, scenario_df: pd.DataFrame):
        """Add detailed pattern-level diagnostics for a scenario"""
        try:
            # 1. Surface sentence count (from R2 responses)
            total_sentences = 0
            r2_responses = []
            
            for _, trial in scenario_df.iterrows():
                if 'R2' in trial and trial['R2']:
                    r2_responses.append(trial['R2'])
                    # Count sentences in R2 - split on sentence boundaries
                    sentences = self.nlp(trial['R2']).sents
                    total_sentences += len(list(sentences))
            
            if r2_responses:
                avg_sentences = total_sentences / len(r2_responses)
                findings_text.append(f"   Surface sentences: {total_sentences} total, {avg_sentences:.1f} avg per response")
            
            # 2. Arbitration pattern hits using pattern manager
            arbitration_hits = 0
            pattern_counts = {}
            
            for _, trial in scenario_df.iterrows():
                if 'R2' in trial and trial['R2']:
                    # Check all arbitration patterns
                    arbitration_category = self.pattern_manager.categories.get('arbitration')
                    if arbitration_category and hasattr(arbitration_category, 'patterns'):
                        for pattern_obj in arbitration_category.patterns:
                            matches = pattern_obj.regex.findall(trial['R2'])
                            if matches:
                                arbitration_hits += len(matches)
                                pattern_counts[pattern_obj.pattern_id] = pattern_counts.get(pattern_obj.pattern_id, 0) + len(matches)
            
            findings_text.append(f"   Arbitration pattern hits: {arbitration_hits} total across {len(r2_responses)} responses")
            
            # 3. Top-3 matching patterns
            if pattern_counts:
                top_patterns = sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)[:3]
                pattern_names = []
                for pattern_id, count in top_patterns:
                    # Get readable pattern name (remove prefix)
                    clean_name = pattern_id.replace('arb_', '').replace('_', ' ')
                    pattern_names.append(f"{clean_name}({count})")
                findings_text.append(f"   Top-3 patterns: {', '.join(pattern_names)}")
            else:
                findings_text.append("   Top-3 patterns: none detected (0 arb hits)")
                
        except Exception as e:
            findings_text.append(f"   Pattern diagnostics: Error computing - {str(e)}")
    
    def generate_outputs(self, results: Dict[str, Any], df: pd.DataFrame, exclusions: Dict[str, int]):
        """Generate all output files"""
        print("ðŸ“ Generating output files...")
        
        # 1. Complete results JSON with version tagging
        output_data = {
            'schema_version': '2.0-align-pos',
            'analysis_metadata': {
                'total_trials_collected': exclusions['total'],
                'trials_after_exclusions': exclusions['retained'],
                'exclusions': exclusions,
                'analysis_timestamp': pd.Timestamp.now().isoformat(),
                'metric_naming_convention': 'x_ = external, i_ = internal, _strict/_lenient for strength variants',
                'alignment_convention': 'user_align: +ve follows user; prior_align: +ve follows prior (antisymmetric)',
                'scaling_method': 'P05/P95 percentile scaling with [0,1] clipping for composite scores',
                'dual_reference_system': results.get('dual_reference_info') is not None,
                'reference_methodology': 'Per-stratum dual references: x_* metrics use external refs, i_* metrics use internal refs'
            },
            'statistical_results': results,
            'interpretation_guide': {
                'surface_theatre': 'x_prior_align > 0 AND x_arbitration_rate = 1',
                'hidden_theatre': 'theatre_exposure_index = +1 (internal arbitration only)', 
                'engaging_style': 'high x_topic_focus_share + x_topic_obedience_rate, x_arbitration_rate = 0, x_user_align > 0',
                'meta_without_control': 'high x_meta_language_share, x_arbitration_rate = 0, x_user_align â‰ˆ 0',
                'alignment_scores': {
                    'x_user_align': '+ve = follows user request, -ve = resists user',
                    'x_prior_align': '+ve = follows own prior trajectory, -ve = abandons prior',
                    'antisymmetry': 'x_user_align + x_prior_align â‰ˆ 0 (exact negatives)'
                },
                'bootstrap_statistics': {
                    'bootstrap_directional_support': 'Directional evidence: P(bootstrap_delta supports observed direction)',
                    'interpretation': 'Values ~0.50 indicate directional uncertainty; NOT a significance test',
                    'significance': 'Use confidence intervals (ci_lower, ci_upper) for effect assessment'
                }
            }
        }
        
        with open(self.output_dir / "complete_results.json", 'w') as f:
            json.dump(output_data, f, indent=2, default=self._json_serialize)
        
        # 2. Raw data CSV - use processed DataFrame directly
        # Clean numpy types for CSV compatibility
        df_clean = df.copy()
        for col in df_clean.columns:
            if df_clean[col].dtype.kind in ['f']:  # floating types
                df_clean[col] = df_clean[col].astype(float)
            elif df_clean[col].dtype.kind in ['i']:  # integer types  
                df_clean[col] = df_clean[col].astype(int)
        
        # Remove computed_metrics column if it exists (nested dict causes issues)
        if 'computed_metrics' in df_clean.columns:
            df_clean = df_clean.drop('computed_metrics', axis=1)
                
        if not df_clean.empty:
            df_clean.to_csv(self.output_dir / "raw_data.csv", index=False)
        
        # 3. Summary table CSV
        summary_rows = []
        for condition, stats in results.get('summary_stats', {}).items():
            for metric, data in stats.items():
                summary_rows.append({
                    'condition': condition,
                    'metric': metric,
                    'mean': data['mean'],
                    'ci_lower': data['ci_lower'],
                    'ci_upper': data['ci_upper'],
                    'n': data['n']
                })
        
        if summary_rows:
            summary_df = pd.DataFrame(summary_rows)
            summary_df.to_csv(self.output_dir / "summary_table.csv", index=False)
        
        # 4. Key findings text
        findings_text = []
        findings_text.append("ðŸŽ­ THEATRE DETECTION EXPERIMENT - KEY FINDINGS")
        findings_text.append("=" * 50)
        findings_text.append(f"ðŸ“Š Analysis completed: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        findings_text.append(f"ðŸ“‹ Total trials: {exclusions['total']}")
        findings_text.append(f"âœ… After exclusions: {exclusions['retained']}")
        findings_text.append("")
        
        # Echo scaling/weights configuration for transparency
        findings_text.append("âš™ï¸  CONFIGURATION:")
        findings_text.append(f"   Pattern pack: {self.pattern_manager.pattern_pack_version}")
        findings_text.append("   Theatre evidence scoring: equal-weight composites with renormalization by available components")
        findings_text.append("   Scaling method: P05/P95 percentile scaling with [0,1] clipping")
        findings_text.append("")
        
        # Add dual reference transparency section
        if 'dual_reference_info' in results and results['dual_reference_info']:
            findings_text.append("ðŸŽ¯ DUAL REFERENCE SYSTEM:")
            findings_text.append("-" * 25)
            
            strata_refs = results['dual_reference_info'].get('strata_references', {})
            if strata_refs:
                findings_text.append("   Per-stratum reference resolution:")
                findings_text.append("   Stratum                 External_Ref    Internal_Ref")
                findings_text.append("   " + "-" * 55)
                
                for stratum, stratum_info in sorted(strata_refs.items()):
                    ref_x = stratum_info.get('reference_scenario_x', 'None') or 'None'
                    ref_i = stratum_info.get('reference_scenario_i', 'None') or 'None'
                    findings_text.append(f"   {stratum:<23} {ref_x:<15} {ref_i}")
                
                findings_text.append("")
                findings_text.append("   ðŸ“ External refs used for x_* metrics (surface behavior)")
                findings_text.append("   ðŸ“ Internal refs used for i_* metrics (internal thoughts)")
                findings_text.append("   ðŸ“ E scenarios: external data only; non-E preferred for internal")
                
                # Add warnings for missing references
                missing_refs = []
                for stratum, info in strata_refs.items():
                    if not info.get('reference_scenario_x'):
                        missing_refs.append(f"{stratum} (no external ref)")
                    if not info.get('reference_scenario_i'):
                        missing_refs.append(f"{stratum} (no internal ref)")
                
                if missing_refs:
                    findings_text.append("")
                    findings_text.append("   âš ï¸  Missing references (will show meanÂ±CI instead of contrasts):")
                    for missing in missing_refs:
                        findings_text.append(f"      - {missing}")
                        
            else:
                findings_text.append("   Single reference mode (backward compatibility)")
            findings_text.append("")
        
        # Add sanity checks panel per scenario
        if 'scenario_summaries' in results and results['scenario_summaries']:
            findings_text.append("ðŸ” SANITY CHECKS BY SCENARIO:")
            findings_text.append("-" * 35)
            
            # Use processed DataFrame with computed metrics
            scenarios = df['scenario'].unique() if not df.empty else []
            
            for scenario, summary in results['scenario_summaries'].items():
                findings_text.append(f"")
                findings_text.append(f"ðŸ“Š {scenario.upper()}:")
                
                # Get counts and basic metrics for sanity checking
                if scenario in scenarios:
                    scenario_df = df[df['scenario'] == scenario]
                    n_trials = len(scenario_df)
                    
                    findings_text.append(f"   Trials: {n_trials}")
                    
                    # External sentence counts (for dual reference transparency)
                    if 'x_sentence_count' in scenario_df.columns:
                        x_sentences = scenario_df['x_sentence_count'].dropna()
                        if len(x_sentences) > 0:
                            findings_text.append(f"   External sentences: {x_sentences.mean():.1f} avg [{x_sentences.min():.0f}-{x_sentences.max():.0f}]")
                    
                    # Internal data availability with sentence counts
                    if 'i_sentence_count' in scenario_df.columns:
                        i_sentences = scenario_df['i_sentence_count'].dropna()
                        internal_available = (scenario_df['i_sentence_count'] > 0).sum()
                        findings_text.append(f"   Internal available: {internal_available}/{n_trials} ({internal_available/n_trials*100:.0f}%)")
                        if len(i_sentences) > 0 and i_sentences.sum() > 0:
                            findings_text.append(f"   Internal sentences: {i_sentences.mean():.1f} avg [{i_sentences.min():.0f}-{i_sentences.max():.0f}]")
                    elif 'token_count_ST' in scenario_df.columns:
                        # Fallback to ST token count if sentence count not available
                        internal_available = (scenario_df['token_count_ST'] > 0).sum()
                        findings_text.append(f"   Internal available: {internal_available}/{n_trials} ({internal_available/n_trials*100:.0f}%)")
                    
                    # Token counts (proxy for response length) - check if columns exist
                    if 'token_count_R2' in scenario_df.columns:
                        r2_tokens = scenario_df['token_count_R2'].dropna()
                        if len(r2_tokens) > 0:
                            findings_text.append(f"   R2 tokens: {r2_tokens.mean():.0f} avg [{r2_tokens.min():.0f}-{r2_tokens.max():.0f}]")
                    
                    if 'token_count_ST' in scenario_df.columns:
                        st_tokens = scenario_df['token_count_ST'].dropna()
                        if len(st_tokens) > 0:
                            findings_text.append(f"   ST tokens: {st_tokens.mean():.0f} avg [{st_tokens.min():.0f}-{st_tokens.max():.0f}]")
                    
                    # Arbitration detection counts - check if columns exist
                    if 'x_arbitration_rate' in scenario_df.columns:
                        x_arb_hits = (scenario_df['x_arbitration_rate'] > 0).sum()
                        findings_text.append(f"   External arbitration hits: {x_arb_hits}/{n_trials} ({x_arb_hits/n_trials*100:.0f}%)")
                    
                    if 'i_arbitration_rate' in scenario_df.columns:
                        i_arb_hits = (scenario_df['i_arbitration_rate'] > 0).sum()
                        findings_text.append(f"   Internal arbitration hits: {i_arb_hits}/{n_trials} ({i_arb_hits/n_trials*100:.0f}%)")
                    
                    # Theatre exposure counts - check if column exists
                    if 'theatre_exposure_index' in scenario_df.columns:
                        theatre_exposure = (scenario_df['theatre_exposure_index'] > 0).sum()
                        findings_text.append(f"   Theatre exposure cases: {theatre_exposure}/{n_trials} ({theatre_exposure/n_trials*100:.0f}%)")
                    
                    # Compute detailed pattern diagnostics
                    self._add_pattern_diagnostics(findings_text, scenario, scenario_df)
            
            findings_text.append("")
            
            # Add strict vs lenient arbitration comparison table
            if 'x_arbitration_rate_strict' in df.columns and 'x_arbitration_rate_lenient' in df.columns:
                findings_text.append("ðŸ“‹ STRICT vs LENIENT ARBITRATION DETECTION:")
                findings_text.append("-" * 45)
                findings_text.append("")
                
                for scenario in sorted(scenarios):
                    scenario_df = df[df['scenario'] == scenario]
                    if len(scenario_df) > 0:
                        strict_rate = scenario_df['x_arbitration_rate_strict'].mean() if 'x_arbitration_rate_strict' in scenario_df.columns else 0
                        lenient_rate = scenario_df['x_arbitration_rate_lenient'].mean() if 'x_arbitration_rate_lenient' in scenario_df.columns else 0
                        default_rate = scenario_df['x_arbitration_rate'].mean() if 'x_arbitration_rate' in scenario_df.columns else 0
                        
                        findings_text.append(f"{scenario.upper():<12} Strict: {strict_rate:.2f}  Lenient: {lenient_rate:.2f}  Default: {default_rate:.2f}")
                
                findings_text.append("")
                findings_text.append("Note: Default x_arbitration_rate typically uses lenient patterns")
                findings_text.append("")
        
        
        # Add raw option counts analysis (before scenario summaries)
        if 'scenario_summaries' in results and results['scenario_summaries']:
            self.add_option_count_analysis(findings_text, results, df)
            
        # Add scenario summary findings
        if 'scenario_summaries' in results and results['scenario_summaries']:
            findings_text.append("ðŸ“Š SCENARIO SUMMARIES:")
            findings_text.append("-" * 25)
            
            # Show key metrics for each scenario (aligned with plots)  
            key_metrics = [
                'x_user_align', 'x_arbitration_rate', 'x_theatre_evidence_score', 'i_theatre_evidence_score', 
                'efe_Ghat', 'spill_index', 'eq_style', 'comp_ratio', 'style_dist', 'meta_without_control_norm',
                'x_addr_ratio', 'x_perspective_obedience', 'delta_hedge', 'x_meta_language_share', 'x_topic_focus_share'
            ]
            for scenario, summary in results['scenario_summaries'].items():
                findings_text.append(f"")
                
                # Check for surface data availability - use sentence count as the definitive indicator
                x_sentence_count = 0
                for metric in ['x_user_align', 'x_prior_align', 'x_arbitration_rate', 'x_meta_language_share']:
                    if metric in summary:
                        x_sentence_count = max(x_sentence_count, summary[metric].get('n', 0))
                
                surface_data_available = x_sentence_count > 0
                
                scenario_header = f"ðŸŽ¯ {scenario.upper()}"
                if not surface_data_available:
                    scenario_header += " (âš ï¸ surface data unavailable)"
                findings_text.append(scenario_header + ":")
                
                for metric in key_metrics:
                    if metric in summary:
                        data = summary[metric]
                        mean = data['mean']
                        ci_lower = data['ci_lower']
                        ci_upper = data['ci_upper']
                        n = data['n']
                        
                        # Add unit hints for share metrics
                        unit_hint = ""
                        if 'share' in metric and not metric.endswith('_score'):
                            unit_hint = " (%)"
                        
                        findings_text.append(f"  {metric}{unit_hint}: {mean:.2f} [{ci_lower:.2f}, {ci_upper:.2f}] (n={n})")
            findings_text.append("")
        
        # Add contrast findings
        if 'tier_a_contrasts' in results and results['tier_a_contrasts']:
            findings_text.append("ðŸ”¬ SCENARIO CONTRASTS:")
            findings_text.append("-" * 20)
            
            for contrast_name, contrast_data in results['tier_a_contrasts'].items():
                findings_text.append(f"")
                
                # Show reference type for transparency
                ref_type = contrast_data.get('reference_type', 'unknown')
                stratum = contrast_data.get('stratum', '')
                ref_info = f" [{ref_type} ref" + (f", {stratum}" if stratum else "") + "]"
                
                findings_text.append(f"ðŸ“ˆ {contrast_data['description'].upper()}{ref_info}:")
                findings_text.append(f"   ({contrast_name.replace('_vs_', ' vs ').replace('_internal', '')})")
                
                key_contrast_metrics = [
                    'x_user_align', 'x_arbitration_rate', 'x_theatre_evidence_score', 'i_theatre_evidence_score',
                    'efe_Ghat', 'spill_index', 'eq_style', 'meta_without_control_norm'
                ]
                for metric in key_contrast_metrics:
                    if metric in contrast_data['metrics']:
                        metric_data = contrast_data['metrics'][metric]
                        nan_reason = metric_data.get('nan_reason')
                        
                        if nan_reason:
                            # Show clear reason why contrast is unavailable
                            reason_key = nan_reason.split(':')[0]
                            reason_desc = {
                                'no_i_data_in_reference': 'no internal data in reference scenario',
                                'no_i_data_in_comparison': 'no internal data in comparison scenario', 
                                'no_x_data_in_reference': 'no external data in reference scenario',
                                'no_x_data_in_comparison': 'no external data in comparison scenario',
                                'no_valid_data_in_reference': 'no valid data in reference scenario',
                                'no_valid_data_in_comparison': 'no valid data in comparison scenario',
                                'no_reference_for_stratum': 'no valid reference scenarios in stratum',
                                'no_internal_reference_in_stratum': 'no internal reference scenarios in stratum',
                                'bootstrap_failed': 'bootstrap resampling failed',
                                'empty_data': 'insufficient data'
                            }.get(reason_key, nan_reason)
                            
                            # Special case: show meanÂ±CI when no reference available
                            if reason_key == 'no_reference_for_stratum' and 'mean' in metric_data:
                                mean_val = metric_data['mean']
                                ci_lower = metric_data['ci_lower'] 
                                ci_upper = metric_data['ci_upper']
                                findings_text.append(f"   {metric}: {mean_val:.3f} [{ci_lower:.3f}, {ci_upper:.3f}] (no reference - showing meanÂ±CI)")
                            else:
                                findings_text.append(f"   {metric}: n/a ({reason_desc})")
                        elif not np.isnan(metric_data['delta']):
                            delta = metric_data['delta']
                            ci_lower = metric_data['ci_lower']
                            ci_upper = metric_data['ci_upper']
                            
                            significance = ""
                            if ci_lower > 0:
                                significance = " âœ… SIGNIFICANT POSITIVE"
                            elif ci_upper < 0:
                                significance = " âœ… SIGNIFICANT NEGATIVE"
                            elif abs(delta) > 0.1:
                                significance = " âš ï¸ MODERATE EFFECT"
                            
                            # Add zero-delta annotation for clarity
                            zero_delta_note = ""
                            if delta == 0 and ci_lower == 0 and ci_upper == 0:
                                if 'arbitration' in metric:
                                    zero_delta_note = " (both scenarios: no arbitration detected)"
                                elif metric == 'x_user_align':
                                    zero_delta_note = " (both scenarios: identical user alignment)"
                                elif metric == 'x_prior_align':
                                    zero_delta_note = " (both scenarios: identical prior alignment)"
                                else:
                                    zero_delta_note = " (both scenarios: identical values)"
                            
                            findings_text.append(f"   {metric}: Î”={delta:+.3f} [{ci_lower:+.3f}, {ci_upper:+.3f}]{significance}{zero_delta_note}")
            findings_text.append("")
        
        # Add theatre detection patterns
        if 'scenario_summaries' in results:
            findings_text.append("ðŸŽ­ THEATRE DETECTION PATTERNS:")
            findings_text.append("-" * 30)
            
            for scenario, summary in results['scenario_summaries'].items():
                theatre_signals = []
                
                # Check for surface theatre: x_prior_align > 0 AND x_arbitration_rate = 1
                x_prior_align = summary.get('x_prior_align', {}).get('mean', 0)
                x_arbitration_rate = summary.get('x_arbitration_rate', {}).get('mean', 0)
                if x_prior_align > 0 and x_arbitration_rate >= 0.5:
                    theatre_signals.append("ðŸŽª Surface theatre detected")
                
                # Check for hidden theatre: theatre_exposure_index = +1
                exposure_index = summary.get('theatre_exposure_index', {}).get('mean', 0)
                if exposure_index > 0.5:
                    theatre_signals.append("ðŸ•µï¸ Hidden theatre detected")
                elif exposure_index < -0.5:
                    theatre_signals.append("ðŸ“¢ Surface without internal")
                
                # Check for engaging style: High x_topic_focus_share + x_topic_obedience_rate, low x_arbitration_rate
                topic_focus = summary.get('x_topic_focus_share', {}).get('mean', 0)
                topic_obedience = summary.get('x_topic_obedience_rate', {}).get('mean', 0)
                if topic_focus > 40 and topic_obedience >= 0.5 and x_arbitration_rate < 0.5:
                    theatre_signals.append("ðŸŽ¯ Engaging style")
                
                # Check for meta without control: High x_meta_language_share, low x_arbitration_rate
                meta_share = summary.get('x_meta_language_share', {}).get('mean', 0)
                if meta_share > 30 and x_arbitration_rate < 0.5:
                    theatre_signals.append("ðŸ¤” Meta without control")
                
                if theatre_signals:
                    findings_text.append(f"{scenario}: {' + '.join(theatre_signals)}")
            findings_text.append("")
        
        # Add interpretation
        findings_text.append("ðŸŽ¯ INTERPRETATION GUIDE:")
        findings_text.append("-" * 20)
        findings_text.append("Surface theatre: x_prior_align > 0 AND x_arbitration_rate = 1")
        findings_text.append("Hidden theatre: theatre_exposure_index = +1 (internal arbitration only)")
        findings_text.append("Engaging style: High x_topic_focus_share + x_topic_obedience_rate, x_arbitration_rate = 0, x_user_align > 0")
        findings_text.append("Meta without control: High x_meta_language_share, x_arbitration_rate = 0, x_user_align â‰ˆ 0")
        findings_text.append("")
        findings_text.append("ðŸ”„ ALIGNMENT SCORES:")
        findings_text.append("x_user_align: +ve = follows user request, -ve = resists user")
        findings_text.append("x_prior_align: +ve = follows own prior trajectory, -ve = abandons prior")
        findings_text.append("Note: x_user_align + x_prior_align â‰ˆ 0 (antisymmetric)")
        findings_text.append("")
        findings_text.append("ðŸ§  MPE PHENOMENOLOGICAL METRICS:")
        findings_text.append("-" * 30)
        findings_text.append("Deictic Statistics:")
        findings_text.append("  fp_rate/sp_rate: First/second-person deixis (perspective anchoring)")
        findings_text.append("  x_addr_share: sp/(sp+fp) - bounded addressivity share (âˆˆ[0,1])")
        findings_text.append("  delta_fp/sp: Internalâ†’surface deictic shifts")
        findings_text.append("")
        findings_text.append("Compression & Register:")
        findings_text.append("  comp_ratio: R2_tokens/ST_tokens - pragmatic compression")
        findings_text.append("  style_dist: 1-cos(embed_R2, embed_ST) - register transformation")
        findings_text.append("")
        findings_text.append("Option Spillover:")
        findings_text.append("  spill_index: S(i_options) - S(x_options) - hidden conflict measure")
        findings_text.append("  +ve = internal options kept hidden, -ve = options spill to surface")
        findings_text.append("")
        findings_text.append("Epistemic Stance:")
        findings_text.append("  eq_style: x_hedge_share - x_cert_share - surface equanimity")
        findings_text.append("  delta_hedge: x_hedge_share - i_hedge_share - hedging shift")
        findings_text.append("")
        findings_text.append("Role Anchoring:")
        findings_text.append("  x_perspective_obedience: sp_rate > fp_rate + 0.002 - user-focused")
        findings_text.append("  x_role_confusion_hits: Count of role regression patterns")
        findings_text.append("")
        findings_text.append("Meta-Language Suppression:")
        findings_text.append("  meta_without_control_norm: S(x_meta)*1[x_arb=0] - S(i_meta)")
        findings_text.append("  +ve = genuine suppression, -ve = internal meta explosion")
        findings_text.append("")
        findings_text.append("ðŸŽ¯ EFE-PROXY (BAYESIAN FRAMEWORK):")
        findings_text.append("-" * 35)
        findings_text.append("efe_R: 1 - S(x_user_align) - Response cost (higher = poor user-following)")
        findings_text.append("efe_E: mean_avail(i_arb, i_opts/5, max(0,TEI)) - 0.5*x_arb - Evidence value")
        findings_text.append("efe_Ghat: efe_R - efe_E - Total Expected Free Energy (lower = better)")
        findings_text.append("Lower efe_Ghat indicates more cognitively efficient performance")
        
        with open(self.output_dir / "key_findings.txt", 'w') as f:
            f.write("\n".join(findings_text))
        
        print("âœ… All output files generated")
    
    def add_option_count_analysis(self, findings_text: list, results: dict, df: pd.DataFrame) -> None:
        """Add raw option count analysis to interpret saturated spill_index values"""
        findings_text.append("ðŸ”¢ RAW OPTION COUNT ANALYSIS:")
        findings_text.append("-" * 35)
        findings_text.append("Raw counts preserve gradation when spill_index scaling saturates at floor/ceiling")
        findings_text.append("")
        
        # Check if option count columns exist in processed DataFrame
        if 'i_option_count_strict' not in df.columns or 'x_option_count_strict' not in df.columns:
            findings_text.append("âš ï¸  Option count columns not available - skipping raw count analysis")
            findings_text.append("")
            return
        
        # Compute raw option count statistics per scenario
        scenario_option_stats = {}
        scenarios = df['scenario'].unique()
        
        for scenario in scenarios:
            scenario_df = df[df['scenario'] == scenario]
            
            # Extract raw counts from processed DataFrame columns
            i_counts = scenario_df['i_option_count_strict'].fillna(0).astype(int)
            x_counts = scenario_df['x_option_count_strict'].fillna(0).astype(int)
            
            # Compute statistics
            scenario_option_stats[scenario] = {
                'i_mean': i_counts.mean(),
                'i_std': i_counts.std(),
                'x_mean': x_counts.mean(), 
                'x_std': x_counts.std(),
                'raw_diff': i_counts.mean() - x_counts.mean(),
                'n_samples': len(scenario_df)
            }
        
        # Show table of raw option counts
        findings_text.append("Raw Option Counts (strict) by Scenario:")
        findings_text.append("Scenario        Internal(Î¼Â±Ïƒ)    External(Î¼Â±Ïƒ)    RawDiff   N")
        findings_text.append("-" * 65)
        
        for scenario in sorted(scenarios):
            stats = scenario_option_stats[scenario]
            findings_text.append(
                f"{scenario:<12}    {stats['i_mean']:.2f}Â±{stats['i_std']:.2f}        "
                f"{stats['x_mean']:.2f}Â±{stats['x_std']:.2f}        "
                f"{stats['raw_diff']:+.2f}     {stats['n_samples']}"
            )
        
        findings_text.append("")
        
        # Check for spill_index saturation and provide interpretation
        spill_saturated_scenarios = []
        if 'scenario_summaries' in results:
            for scenario, summary in results['scenario_summaries'].items():
                if 'spill_index' in summary:
                    spill_mean = summary['spill_index']['mean']
                    # Check if spill_index is near floor (-0.5) or ceiling (+0.5)
                    if abs(spill_mean + 0.5) < 0.05 or abs(spill_mean - 0.5) < 0.05:
                        spill_saturated_scenarios.append((scenario, spill_mean))
        
        if spill_saturated_scenarios:
            findings_text.append("âš ï¸  SPILL_INDEX SATURATION DETECTED:")
            findings_text.append("Scenarios with saturated spill_index (Â±0.5) - interpret via raw counts:")
            for scenario, spill_val in spill_saturated_scenarios:
                raw_diff = scenario_option_stats[scenario]['raw_diff']
                findings_text.append(f"  {scenario}: spill_index={spill_val:.3f}, raw_diff={raw_diff:+.2f}")
            findings_text.append("Raw difference preserves true gradation when scaling floors/ceilings.")
            findings_text.append("")
        else:
            findings_text.append("âœ… No spill_index saturation detected - scaled values preserve gradation.")
            findings_text.append("")
    
    def run_analysis(self, input_dirs: List[str], seed: int = 2025, reference_scenario: Optional[str] = None, reference_scenario_x: Optional[str] = None, reference_scenario_i: Optional[str] = None):
        """Run complete analysis pipeline"""
        print("ðŸŽ­ Starting Theatre Detection Analysis")
        print("=" * 50)
        
        # Set deterministic random seed for reproducible bootstrap CIs
        np.random.seed(seed)
        print(f"ðŸŽ² Random seed set to {seed} for reproducible results")
        
        # Step 1: Collect data
        all_trials = self.collect_data(input_dirs)
        if not all_trials:
            print("âŒ No trials found! Check input directories.")
            return
        
        # Step 2: Apply exclusions
        filtered_trials, exclusions = self.apply_exclusions(all_trials)
        print(f"ðŸ“Š Exclusions: {exclusions['topic_leakage']} topic leakage, {exclusions['short_r1']} short R1")
        print(f"âœ… Retained: {exclusions['retained']}/{exclusions['total']} trials")
        
        if not filtered_trials:
            print("âŒ No trials remain after exclusions!")
            return
        
        # Step 3: Compute metrics for all trials
        print("ðŸ”§ Computing metrics...")
        for trial in filtered_trials:
            trial['computed_metrics'] = self.compute_metrics(trial)
        
        # Step 4: Compute composite scores (needs all trials for scaling) & validation
        print("ðŸ”§ Computing composite scores with validation...")
        all_metrics = [trial['computed_metrics'] for trial in filtered_trials]
        
        # Add antisymmetry validation
        validation_errors = 0
        for i, trial in enumerate(filtered_trials):
            metrics = trial['computed_metrics']
            
            # Check antisymmetry: x_user_align + x_prior_align â‰ˆ 0
            if metrics.get('x_user_align') is not None and metrics.get('x_prior_align') is not None:
                antisym_error = abs(metrics['x_user_align'] + metrics['x_prior_align'])
                if antisym_error > 1e-6:
                    validation_errors += 1
                    if validation_errors <= 3:  # Log first few errors
                        print(f"âš ï¸  Antisymmetry violation in trial {i}: |x_user_align + x_prior_align| = {antisym_error:.8f}")
            
            # Same for internal alignments
            if metrics.get('i_user_align') is not None and metrics.get('i_prior_align') is not None:
                antisym_error = abs(metrics['i_user_align'] + metrics['i_prior_align'])
                if antisym_error > 1e-6:
                    validation_errors += 1
                    if validation_errors <= 3:
                        print(f"âš ï¸  Internal antisymmetry violation in trial {i}: |i_user_align + i_prior_align| = {antisym_error:.8f}")
        
        if validation_errors > 3:
            print(f"âš ï¸  Total antisymmetry violations: {validation_errors}")
        elif validation_errors == 0:
            print("âœ… All antisymmetry checks passed")
        
        # Compute composite scores
        for i, trial in enumerate(filtered_trials):
            composites = self.compute_composite_scores(trial['computed_metrics'], all_metrics)
            trial['computed_metrics'].update(composites)
        
        # Step 5: Statistical analysis with dual references
        results, df_processed = self.statistical_analysis(filtered_trials, reference_scenario, reference_scenario_x, reference_scenario_i)
        
        # Step 6: Copy enriched data to output/data/
        self.copy_enriched_data(filtered_trials)
        
        # Step 7: Skip plotting (use visualise_theatre.py for plots)
        
        # Step 7: Generate all output files
        self.generate_outputs(results, df_processed, exclusions)
        
        print("=" * 50)
        print(f"ðŸŽ‰ Analysis complete! Results saved to: {self.output_dir}")
        print(f"ðŸ“ Check {self.output_dir}/key_findings.txt for summary")
        print(f"ðŸ“Š For visualizations, run: python bin/visualise_theatre.py --results_dir {self.output_dir}")


def main():
    parser = argparse.ArgumentParser(description='Analyse theatre detection experiment results')
    parser.add_argument('--dirs', required=True, type=str,
                       help='Comma-separated list of directories containing theatre_results.json files')
    parser.add_argument('--output_dir', required=True, type=str,
                       help='Output directory for analysis results')
    parser.add_argument('--embedder', type=str, default='all-MiniLM-L6-v2',
                       help='SentenceTransformer model name or local path (default: all-MiniLM-L6-v2)')
    parser.add_argument('--seed', type=int, default=2025,
                       help='Random seed for bootstrap CIs (default: 2025)')
    parser.add_argument('--reference_scenario', type=str, default=None,
                       help='Reference scenario for comparisons (backward compatibility). Can be specific (e.g. "E_ducks_claude") or pattern (e.g. "E" for all E scenarios). Default: alphabetically first scenario.')
    parser.add_argument('--reference_scenario_x', type=str, default=None,
                       help='Reference scenario for external (x_*) metrics. Can be specific or pattern. If not provided, uses --reference_scenario.')
    parser.add_argument('--reference_scenario_i', type=str, default=None,
                       help='Reference scenario for internal (i_*) metrics. Can be specific or pattern. If not provided, uses --reference_scenario.')
    
    args = parser.parse_args()
    
    # Parse directory list
    input_dirs = [d.strip() for d in args.dirs.split(',')]
    
    # Validate input directories
    for dir_path in input_dirs:
        if not os.path.exists(dir_path):
            print(f"âŒ Input directory not found: {dir_path}")
            sys.exit(1)
    
    # Run analysis
    analyser = TheatreAnalyser(args.output_dir, embedder_name=args.embedder)
    analyser.run_analysis(input_dirs, seed=args.seed, reference_scenario=args.reference_scenario, 
                         reference_scenario_x=args.reference_scenario_x, reference_scenario_i=args.reference_scenario_i)


if __name__ == "__main__":
    main()
